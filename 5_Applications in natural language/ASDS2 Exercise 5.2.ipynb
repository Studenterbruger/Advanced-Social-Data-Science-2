{"cells":[{"cell_type":"code","execution_count":null,"id":"xOVuadtPdzmg","metadata":{"id":"xOVuadtPdzmg"},"outputs":[],"source":["# Installing datasets and transformers for Colab\n","!pip install datasets==2.2.1 transformers==4.19.1"]},{"cell_type":"code","execution_count":null,"id":"Of51fSjSd8gW","metadata":{"id":"Of51fSjSd8gW","tags":[]},"outputs":[],"source":["import os\n","import numpy as np\n","from collections import Counter\n","import torch\n","import datasets\n","datasets.logging.set_verbosity_error()\n","from datasets import load_metric\n","from google.colab import drive\n","from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","\n","# # uncomment if CAN'T CONNECT TO GPU (it happens...)\n","# import psutil\n","# import platform"]},{"cell_type":"code","execution_count":null,"id":"zW0R9HiKeAlU","metadata":{"id":"zW0R9HiKeAlU"},"outputs":[],"source":["# GPU housekeeping code: you do not need to modify anything, simply\n","# read through it to understand what is going on, and run as is\n","\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","# a helper function to format byte counts into KB, MB and so on\n","def bytes_format(b):\n","    if b < 1000:\n","              return f'{b} B'\n","    elif b < 1000000:\n","        return f'{round(float(b/1000),2)} KB'\n","    elif b < 1000000000:\n","        return f'{round(float(b/1000000),2)} MB'\n","    else:\n","        return f'{round(float(b/1000000000),2)} GB'\n","\n","# a helper function to check the amount of available memory\n","def memory_report():\n","  if device!='cpu':\n","    print(f\"GPU available: {torch.cuda.get_device_name()}\")\n","    #print(torch.cuda.memory_summary())\n","    total = torch.cuda.get_device_properties(0).total_memory\n","    reserved = torch.cuda.memory_reserved(0)\n","    allocated = torch.cuda.memory_allocated(0)\n","  #  free = reserved-allocated  # free inside memory_reserved\n","    print(f\"Total cuda memory: {bytes_format(total)}, reserved: {bytes_format(reserved)}, allocated: {bytes_format(allocated)}\")\n","  else:\n","    # Print total memory available on CPU\n","    print(f'Device is CPU {platform.processor()}. GPU is not available rn')\n","    total_memory = psutil.virtual_memory().total\n","    print(f\"Total CPU memory: {bytes_format(total_memory)}\")\n","\n","memory_report()"]},{"cell_type":"markdown","id":"781eed6f-50a7-4d2e-af45-8e45717ad005","metadata":{"id":"781eed6f-50a7-4d2e-af45-8e45717ad005"},"source":["# Exercise: sentence classification\n","\n","In this exercise, we will focus a bit more deeply on using supervised machine learning for classifying sentences (and other short documents). Of course, classifying short documents is what we have been doing throughout section 4 and 5 of the course. Here, we will look at irony prediction and stance detection as examples of tasks that go beyond sentiment classification. We will (1) take a closer look at annotations to understand the difficulty of coding (annotating) text, even for human coders; and (2) evaluate the performance of a fine-tuned, pre-trained BERT model on these tasks.\n","\n","We will once again run this notebook on Google Colab (as in exercise set 4.3), so that we can use GPUs for fine-tuning BERT. Note that below, you will need to use a file with hand-coded annotations that you create yourself. This means you will have to give the notebook access to the Google drive folder where you store this file; the code for that is included below. "]},{"cell_type":"markdown","id":"fc5f7b17-984c-440b-a27b-b336d2ddfa91","metadata":{"id":"fc5f7b17-984c-440b-a27b-b336d2ddfa91"},"source":["# 1. Understand the irony detection data\n","\n","Download the `tweet_eval` data set for the irony detection task. The whole suite of `tweet_eval` data sets is described [here](https://huggingface.co/datasets/tweet_eval); select \"irony\" as the subset to see examples of the irony detection task.\n","\n","1. How many tweets are in the training and validation set? How many are in the irony and no-irony categories?"]},{"cell_type":"code","execution_count":null,"id":"34d60414-5864-4d4c-ad0a-cd78c7dfcc7c","metadata":{"id":"34d60414-5864-4d4c-ad0a-cd78c7dfcc7c"},"outputs":[],"source":["# load the tweet_eval irony datasets\n","train_dataset = \n","val_dataset = "]},{"cell_type":"markdown","id":"9170f69c-8c6e-4f54-9171-f9c116296e83","metadata":{"id":"9170f69c-8c6e-4f54-9171-f9c116296e83"},"source":["2. Have a look at the [paper](https://aclanthology.org/S18-1005.pdf) that explains this dataset and task:\n","\n","Van Hee, Cynthia, Els Lefever, and VÃ©ronique Hoste. \"Semeval-2018 task 3: Irony detection in English tweets.\" In Proceedings of The 12th International Workshop on Semantic Evaluation, pp. 39-50. 2018.\n","\n","How were the tweets for this task selected (before being hand-coded)? How could this influence the performance of the task on other tweets? Discuss this with a neighbor, if you can."]},{"cell_type":"markdown","id":"3d1a6bc2-f842-4f01-8c9a-ce412f75ecb1","metadata":{"id":"3d1a6bc2-f842-4f01-8c9a-ce412f75ecb1"},"source":["3. Hand-annotate the irony of 50 randomly selected tweets yourself. Calculate the [Cohen's kappa](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html) for interrater agreement between yourself and the original coder(s). Compare your annotations to those of the trained coder, and look at the disagreements: how many of them would you consider to be mistakes on your end, mistakes on their end, or tweets whose true irony label is simply unclear?\n","\n","**Comments on the hand-annotating**\n","\n","Normally, we would export a sample of the data without the labels, since of course we don't want to see the original coder's labels as we are doing our own annotations. Then, we would merge the original labels back in using their IDs. However, since data wrangling isn't the focus here, you can just export the data with labels to an excel file, and \"hide\" the original labels are you do your own annotations. We provide some skeleton code below.\n","\n","**Comments on Cohen's Kappa Score**\n","\n","The kappa score is a number between -1 and 1. Neuendorf (2002) rates levels of annotator agreement (also called intercoder reliability or IRC) as follows:\n","\n","- Above .8 is nearly perfect agreement\n","- Between  0.61 and 0.80 as substantial agreement \n","- Between 0.41 and 0.60 as moderate agreement\n","- Between 0.21 and 0.40 as fair agreement\n","- Below 0.2 is slight agreement\n","\n","Zero or lower means no agreement (practically random labels). "]},{"cell_type":"code","execution_count":null,"id":"zPcOQb3GQMH1","metadata":{"id":"zPcOQb3GQMH1"},"outputs":[],"source":["# In order to work in a directory on our Google drive, we first have to mount our drive\n","\n","# NB: The code will trigger permission prompts \n","drive.mount('/content/drive')\n","\n","# Setting path to current working directory\n","path = '/content/drive/My Drive/path/to/folder/where/you/will/put/your/annotation/file'\n","#change directory\n","os.chdir(path)"]},{"cell_type":"code","execution_count":null,"id":"vtRdztElyUTL","metadata":{"id":"vtRdztElyUTL"},"outputs":[],"source":["# Selecting random sample of 50 tweets\n","\n","# Setting a seed to make sure that everyone gets the same sample \n","seed=42\n","\n","# Converting dataset to a pandas dataframe\n","df_sample=pd.DataFrame(train_dataset)\n","\n","# Taking sample \n","df_sample=df_sample.sample(FILLINTHEBLANK, random_state=seed)\n","\n","# Saving sample to excel-file that can be manually annotated \n","# We open the xlsx-file in our drive as a google sheets file and manually annotate the data in a new column called \"my_label\"\n","# During annotation we hide the column \"label\" containing the existing annotations  \n","df_sample.to_excel(\"irony_annotation.xlsx\", index = False)"]},{"cell_type":"markdown","id":"2cc995ef-46cc-49ba-b11c-92a54a01c577","metadata":{"id":"2cc995ef-46cc-49ba-b11c-92a54a01c577"},"source":["# 2. Finetune BERT for irony detection\n","\n","Building on the work that you did in exercise set 4.3, fine-tuning BERT for the `tweet_eval` sentence classification task, now it's time to fine-tune BERT for the irony detection task. You can use your own code or the solutions code from the previous exercise to accomplish this. That means:\n","\n","1. Setting up `transformer` for the medium-size BERT model \"prajjwal1/bert-medium\"\n","2. Tokenizing the tweets with the tokenizer associated with our masked language model, using the [AutoTokenizer](https://huggingface.co/docs/transformers/v4.19.0/en/model_doc/auto#transformers.AutoTokenizer).\n","3. Initializing the pre-trained model using the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.19.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification) module, setting it up for classification into the right number of classes, and then moving it to GPU.\n","4. Preparing a [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) object and a function that computes the F1 evaluation metric, to be passed as arguments to the Trainer. Set the number of epochs to 5 (or 2-3 if you don't have a GPU) and the batch size to 16 in the training arguments.\n","5. Creating a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) object and passing it the model, the training arguments (args), the pre-defined metric (compute_metric), the train_dataset and eval_dataset, as well as the tokenizer object.\n","6. Training the model using its `.train()` method.\n","\n","What kind of performance do you see in terms of F1? How does this compare to the F1 scores reported in the paper?"]},{"cell_type":"code","execution_count":null,"id":"MAdzn_Gvv298","metadata":{"id":"MAdzn_Gvv298"},"outputs":[],"source":["# 1. Defining the model\n","\n","# NB: Try using bert-small if running on CPU (not GPU)\n","bert_medium = "]},{"cell_type":"code","execution_count":null,"id":"HlHVa9nDvtk5","metadata":{"id":"HlHVa9nDvtk5"},"outputs":[],"source":["# 2. Set up the tokenizer we want to use\n","tokenizer = \n","\n","# Moving tokenizer to work on GPU \n","tokenizer.to_device = device\n","\n","# Apply the tokenizer to each row in the dataset\n","tokenized_train_dataset =\n","tokenized_val_dataset ="]},{"cell_type":"code","execution_count":null,"id":"36ec0718-d4b1-410d-9819-872d45f1ba10","metadata":{"id":"36ec0718-d4b1-410d-9819-872d45f1ba10"},"outputs":[],"source":["# 3.  initializing the pre-trained model using the AutoModelForSequenceClassification module \n","irony_classifier = \n","\n","# Moving model to GPU\n","irony_classifier.to(device)"]},{"cell_type":"code","execution_count":null,"id":"0D41O6EJAX8z","metadata":{"id":"0D41O6EJAX8z"},"outputs":[],"source":["# 4.  Setting the training arguments\n","# NB:  If your are not connected to the GPU try lowering the number of epochs to 2 or 3  \n","training_args = \n"]},{"cell_type":"code","execution_count":null,"id":"XnN44s5NAomu","metadata":{"id":"XnN44s5NAomu"},"outputs":[],"source":["# 5. Defining the f1 score metric\n","metric = #use the load_metric method from the datasets library to load f1 from sklearn\n","\n","# Defining a function that computes it given a tuple of outputs and labels\n","def compute_f1(eval_pred):\n","    outputs, labels = eval_pred\n","    predictions = np.argmax(outputs, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"]},{"cell_type":"code","execution_count":null,"id":"2NcqQBV4BhEa","metadata":{"id":"2NcqQBV4BhEa"},"outputs":[],"source":["# 5. Defining a trainer object with the information from above \n","trainer = Trainer(FILLINTHEBLANK)\n","\n","# 6. Training and evaluating model using its .train() method\n"]},{"cell_type":"markdown","id":"43cc9bde-933f-4e34-9a0e-450a3d4df96b","metadata":{"id":"43cc9bde-933f-4e34-9a0e-450a3d4df96b"},"source":["# 3. Repeat the exercise for climate stance detection.\n","\n","Download the `tweet_eval` data set for the `stance_climate` task. Also have a look at the relevant [paper](https://aclanthology.org/S16-1003.pdf):\n","\n","Mohammad, Saif, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry. \"Semeval-2016 task 6: Detecting stance in tweets.\" In Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016), pp. 31-41. 2016.\n","\n","Repeat exercise 1 and 2 for this dataset and task.\n","\n","For 1:\n","\n","Looking at your own Cohen's kappa with the original coders, is this a more or less difficult task? (Note: Cohen's kappa can be compared between tasks that have different number of output classes and different degrees of balance, because it takes into accunt the baseline probability that two coders would agree on a label)\n","\n","For 2:\n","\n","This time, since we have three outcome categories, we need to define a slightly difference performance metric. Complete the example code to define an evaluation metric to match the F_avg metric that is used in the paper, which takes the average of the F1 metrics for the categories \"favor\" and \"against\". After fine-tuning BERT, how does this F_avg metric compare to the one you found for the irony task? Would you have expected this given the size of the datasets, the balance in the classes, and the difficulty of the coding task?\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0VJjPzFf3G_Z","metadata":{"id":"0VJjPzFf3G_Z"},"outputs":[],"source":["# Defining a function for finding the average f1 for favor and against labels \n","def compute_f_avg(eval_pred):\n","    outputs, labels = eval_pred \n","    predictions = np.argmax(outputs, axis=-1) \n","    \n","    # Filter labels and predictions for \"favor\" and \"against\" categories\n","    favor_labels = labels[labels == 2]\n","    favor_predictions = predictions[labels == 2]\n","    against_labels = labels[labels == 1]\n","    against_predictions = predictions[labels == 1]\n","    \n","    # Calculating f1 for favor and against\n","    f1_favor = f1_score(favor_labels, favor_predictions, average='weighted', zero_division=0) # The zero_division parameter is set to 0 to handle the case when there are no instances of a particular class.\n","    f1_against = f1_score(against_labels, against_predictions, average='weighted', zero_division=0)\n","    \n","    # Finding average\n","    f_avg = FILLINTHEBLANK\n","    \n","    return {'f_avg': f_avg}"]},{"cell_type":"code","execution_count":null,"id":"06a64dc9-4ad1-4297-9320-2f709704685c","metadata":{"id":"06a64dc9-4ad1-4297-9320-2f709704685c"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1bRkPlYES9CWq-gfFTJW4gg-OF8Op3Ykh","timestamp":1684267623613},{"file_id":"1Xqd1WH8AIBaic3Il0mIi9egeZdey7O-N","timestamp":1684238451001},{"file_id":"1pvtvJVfX2cxjO5r_hKlA-H1PwQuhbLl7","timestamp":1684225770019}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":5}