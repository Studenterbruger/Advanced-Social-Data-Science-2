{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 5492,
     "status": "error",
     "timestamp": 1683236640122,
     "user": {
      "displayName": "Sofie Læbo",
      "userId": "09223936044500351745"
     },
     "user_tz": -120
    },
    "id": "zM7TctsvuIQu",
    "outputId": "fb724724-d830-4b28-e8ba-7537457728a4",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asger\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\advanced_social_data-1nT6mJ2B-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ALSyiN7wuIQ5"
   },
   "source": [
    "# Exercise: Shallow neural networks\n",
    "\n",
    "In this exercise session, you will be defining and training a multiclass logistic regression model (a.k.a. softmax regression or multinomial logit) as a simple neural net in PyTorch. First, we will set up the model. Then, we will load the `tweet_eval` dataset and turn it into a Torch dataset (the type of data input that PyTorch can work with). Next, we will train our network on the data using a variant of gradient descent called Adam. Finally, we will evaluate the model's performance.\n",
    "\n",
    "We will provide you with code snippets to help you get started. Where you see a `FILLINTHEBLANK` in the code, or a line that ends in a `=` that is where you complete the code to make it functional.\n",
    "\n",
    "Note: in order to have this notebook display the Figure in problem 5, you should also download the fil `Net for DLI exercise.drawio.png` from Absalon and store it in the same folder as this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ik6RO3lsuIQ9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Define the PyTorch model\n",
    "\n",
    "Finish the definition of this model.\n",
    "\n",
    "1. Tthe `__init__` method of your model you should define all the layers you are going to use. PyTorch provides a large amount of commonly used layers that are very easy to use. Please refer to the [documentation of PyTorch](https://pytorch.org/docs/stable/nn.html) for a complete list of layers. Here, the `__init__` method should contain one layer that linearly combines the inputs, and then a softmax activation function, which will produce the outputs.\n",
    "2. In the forward pass, the model should do the following:\n",
    " - compute the z values of the node (linear combinations of the inputs, using the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) neural net building block). There should be as many outputs as classes here (since the probability of each class gets to be based on its own linear combination of the features)\n",
    " - pass them through the [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) activation function with `dim=1` to normalize them (squeeze the probabilities of all output classes so that they sum to one)\n",
    " - return the outputs\n",
    "3. Try initializing and inspecting the model as a toy_model instance. It should have 4 features and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EyLR_tNluIQ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class LogisticRegression(torch.nn.Module): # With Pytorch, creating a new model means creating a new class.\n",
    "\n",
    "    # initializing the model with a certain number of input features\n",
    "    # and output classes\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # creating a layer that applies a linear transformation\n",
    "        # to the incoming data: z = wT * x (here x is input tensor and z will be output)\n",
    "        self.linear = torch.nn.Linear(n_features, n_classes) # layer applying linear transformation to input\n",
    "\n",
    "        # creating a softmax activation function\n",
    "        self.activ = torch.nn.Softmax(dim=1) # Creating a softmax-activation - normalizing output to have sum=1\n",
    "\n",
    "    # you have to define the forward() method which will specify the forward propagation:\n",
    "    # how the input values get to the next layer(s)\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # compute the linear z-values for the layer\n",
    "        z = self.linear(inputs) # z-values are computed using the linear transformation on the input\n",
    "\n",
    "        # pass them through the softmax activation function\n",
    "        outputs = self.activ(z) # Outputs are computed using the softmax activation on the z-values\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XuzKbAcyuIRD"
   },
   "source": [
    "This model is a logistic regression model that takes in input data, applies a linear transformation, and passes it through a softmax activation function to predict the probability of an instance belonging to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h1XXYAXluIRE",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting number of input features and classes\n",
    "n_features=4\n",
    "n_classes=3\n",
    "\n",
    "# Initializing a model refering to the class LogisticRegression defined above. \n",
    "toy_model = LogisticRegression(n_features,n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hz6UsDxnuIRH",
    "outputId": "ca8bc21a-b6cd-418c-d2f3-f9fc940a1c58",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definition of the toy_model is: \n",
      " \n",
      " LogisticRegression(\n",
      "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (activ): Softmax(dim=1)\n",
      ")\n",
      "\n",
      "The parameters of the model: \n",
      "\n",
      "The parameter: linear.weight has shape: torch.Size([3, 4])\n",
      "\n",
      "The parameter: linear.bias has shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the toy_model\n",
    "\n",
    "print('The definition of the toy_model is: \\n \\n',toy_model) # Printing the definition of the model\n",
    "\n",
    "toy_model_dict=toy_model.state_dict() # creating a dictionary containing names and values of model parameters\n",
    "\n",
    "print('\\nThe parameters of the model: ')\n",
    "for param in toy_model_dict.keys():\n",
    "    print('\\nThe parameter: {} has shape: {}'.format(param,toy_model_dict[param].shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "erH-Oza-uIRL"
   },
   "source": [
    "# 2. Prepare the *Tweet_eval* dataset\n",
    "\n",
    "This is a data set of tweets that are hand-coded as either negative, neutral or positive\n",
    "\n",
    "## Load the data set\n",
    "\n",
    "1. Have a look at the paper by Rosenthal et al. below. How were the tweets selected? How were they annotated?\n",
    "2. Load the dataset (train and validation splits) from the huggingface library.\n",
    "3. Have a look at the dataset on [huggingface's website](https://huggingface.co/datasets/tweet_eval/viewer/sentiment/train), to get a sense of what's in it.  Also have a look at the size of the training and validation sets, and do a count of the number of training samples with each label, to see whether the data is balanced (all classes represented evenly) or unbalanced.\n",
    "\n",
    "The sentiment task is described in this paper:\n",
    "\n",
    "> Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 Task 4: Sentiment Analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502–518, Vancouver, Canada. Association for Computational Linguistics. [https://aclanthology.org/S17-2088/](https://aclanthology.org/S17-2088/)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wFfe6S0quIRN"
   },
   "source": [
    "**Selection and annotation of tweets**\n",
    "\n",
    "English-language tweets were collected if their content was related to popular current events that were trending on Twitter. \n",
    "\n",
    "Tweets were annotated for sentiment on a 2-point, 3-point and 5-point scale. Annotators both annotated overall sentiment and sentiment towards a topic. All tweet annotations were performed through Crowdflower. \n",
    "\n",
    "The data on Huggingface contains strings from tweets and sentiment scores on a 3-point scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0vg0bA1nuIRO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading traing and validation data \n",
    "train = datasets.load_dataset('tweet_eval', 'sentiment', split='train')\n",
    "val = datasets.load_dataset('tweet_eval', 'sentiment', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jo2j6OcLuIRP",
    "outputId": "3250d98f-03b3-4d27-917b-2299b7a45db3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in training data: 45615\n",
      "Number of tweets in validation data: 2000\n",
      "\n",
      "Number of tweets labeled with 0:  7405\n",
      "Number of tweets labeled with 1:  21542\n",
      "Number of tweets labeled with 2:  18668\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the size of the loaded data:\n",
    "print('Number of tweets in training data:' ,len(train))\n",
    "print('Number of tweets in validation data:', len(val))\n",
    "\n",
    "# Inspecting if data is balanced by counting labels for each class  \n",
    "print('\\nNumber of tweets labeled with 0: ',(val['label']+train['label']).count(0)) \n",
    "print('Number of tweets labeled with 1: ',(val['label']+train['label']).count(1))\n",
    "print('Number of tweets labeled with 2: ',(val['label']+train['label']).count(2)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3un-16UruIRQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Turn it to Torch tensors\n",
    "\n",
    "1. Vectorize the tweet texts with the TfIDF vectorizer from sklearn\n",
    "2. Convert this data to Torch tensors. Note that the output of the sklearn vectorizer is not numpy arrays but scipy matrices, which can be converted with `toarray()` method. Next, you can convert those to torch vectors using the torch[from_numpy](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) method. Finally, you will need to convert the feature tensors to float type with `float()` method.\n",
    "\n",
    "Labels are lists, and so can be converted to numpy arrays with `np.array(mylist)`. Then we also need to turn those into torch vectors. \n",
    "\n",
    "If your computer is struggling with the conversion, simply reduce the amount of training data to a slice (e.g. first 10K examples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "72f81Q6guIRR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorizing the data with TF-IDF corpus\n",
    "vectorizer = TfidfVectorizer()  # the default ngram range is (1,1)\n",
    "\n",
    "train_corpus = train[\"text\"][:10000] # creating training corpus and subsetting to only 10k tweets\n",
    "train_labels = train[\"label\"][:10000] # Defining labels for subset of 10K tweets\n",
    "train_features = vectorizer.fit_transform(train_corpus) # Vectorizing text and learning features for training data \n",
    "\n",
    "val_corpus = val[\"text\"] # Creating corpus and labels for validation set\n",
    "val_labels = val[\"label\"]\n",
    "val_features = vectorizer.transform(val_corpus)# vectorizing text in validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "77hA2P3EuIRT",
    "outputId": "4e223332-6621-45df-eb44-ebca3523bfd8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 18484]) torch.Size([2000, 18484])\n",
      "torch.Size([10000]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# Converting the vectorizer to Torch tensors\n",
    "\n",
    "x_train = torch.from_numpy(train_features.toarray()).float() # toarray() makes features np.arrays and torch.from_numpy makes these torch.tensors\n",
    "y_train = torch.from_numpy(np.array(train_labels))\n",
    "x_test = torch.from_numpy(val_features.toarray()).float()\n",
    "y_test = torch.from_numpy(np.array(val_labels))\n",
    "\n",
    "print(x_train.size(), x_test.size()) # Printing size of test data note that we get size of train and test data and of vocabulary/features \n",
    "print(y_train.size(), y_test.size()) # The number of features are the same for train and test data since we only fitted the vectorizer to train data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VwdveIjRuIRW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Turn the data into Torch Datasets\n",
    "\n",
    "We're still not done with the data preparation! The canonical way to handle data in PyTorch is with objects of the [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset) class. More specifically, we are going to define our own subclass `TweetEvalData` that is a special case of the `Dataset` class. There is an official PyTorch [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "1. You are provided with the code for the class `TweetEvalData` (a subclass of the torch Dataset class) that can contain our `tweet_eval` data. Fill in the parts of the new class definition where provide the features and labels. Next, you will need to complete the code that creates two instances of that class: one for the train data and one for the validation data. The features and labels must come in the form of torch tensors. Luckily, you have just done that in the previous step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "SXbEHdqduIRX",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making a subclass of the torch Datasets class\n",
    "# You don't need to modify this code; just take a look at\n",
    "# what it does and run it\n",
    "\n",
    "class TweetEvalData(torch.utils.data.Dataset): # initiating a class for representing data with three methods\n",
    "\n",
    "    def __init__(self, X, y): # describes how the dataset is initialized; the arguments (when initializing) are the features and labels\n",
    "        self.X = X #any instance of the TweetEvalData class will have an attribute X that contains its features \n",
    "        self.y = y #same with an attribute y that contains its labels\n",
    "\n",
    "    def __getitem__(self, index): # getitem allows to retrieve a datapoint from the dataset by its index.\n",
    "        X = self.X[index] \n",
    "        y = self.y[index].unsqueeze(0) #tensor is unsqueezed to ensure correct shape for training\n",
    "        return X, y # methods returns corresponding data point to input index by an x and y tensor \n",
    "\n",
    "    # a helper to check and return the size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.y) # Returning the number of labels in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing datasets\n",
    "\n",
    "dataset_class_train = TweetEvalData(x_train, y_train) # initiating an instance of the class that contains the train data\n",
    "dataset_class_val = TweetEvalData(x_test, y_test) #initating an instance of the class that contains the test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finally, we need so-called Dataloaders for each of these data sets. As you will see in the example code later on, a Dataloader allows us to easily iterate over samples of data and corresponding labels during training of our model. Create Dataloaders for the train and test sets using `torch.utils.data.DataLoader`, with `batch_size` 64. Batch size is number of samples that are processed before the model is updated while training; each step of our gradient descent is based on one batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "id": "ixw-WPXzuIRY",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initiating dataloaders \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_class_train, batch_size = 64)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_class_val, batch_size = 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Co_e1Py_uIRZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Let's train our neural network!\n",
    "\n",
    "1. Create an instance of our LogisticRegression. The input feature size should correspond to the size of tfidf vectors. We still have 3-class classification.\n",
    "2. Choose a loss function ([CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) and an optimizer, a.k.a. a gradient descent algorithm ([Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate 0.001)\n",
    "3. Complete and run the provided code for training the model across 5 epochs. Is your loss going down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cmqssQ0CuIRa",
    "outputId": "eb590cdb-092a-4553-9bc4-abe3d23e9ce8",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=18484, out_features=3, bias=True)\n",
       "  (activ): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating model to take input vectors with correct size\n",
    "model = LogisticRegression(18484, 3) # n_features = number of features found with tfidf, n_classes=3 labels on 3-point sentiment scale\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nmG2ZdpuuIRb",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining loss function and optimizer\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss() # loss function for tracing loss during training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer to update weights and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "id": "td_LhBn4uIRb",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "22b6e874-7d78-46df-b3a7-341f03d49efe",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.0828479057664324\n",
      "Epoch 1: loss 1.0539012995495158\n",
      "Epoch 2: loss 1.032682115105307\n",
      "Epoch 3: loss 1.0158119463616875\n",
      "Epoch 4: loss 1.0016070854891637\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "for epoch in range(5): # epochs are the number of complete passes through the training dataset \n",
    "    losses = [] # storing the loss for each epoch\n",
    "    for batch_index, (inputs, targets) in enumerate(train_loader): # looping over elements in train_loader\n",
    "\n",
    "        # zero the gradients that are stored from the previous optimization step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the model outputs and the original target (true) labels\n",
    "        outputs = model(inputs) # compute the outputs of the model\n",
    "        targets = torch.flatten(targets) # converting target tensor from shape (batch_size,1) to (batch_size)\n",
    "        targets = targets.type(torch.LongTensor) # Converting to torch.LongTensor as required for loss function\n",
    "    \n",
    "        # compute the loss here\n",
    "        loss = loss_function(outputs,targets) #loss function compares model output and the true labels\n",
    "\n",
    "        # back-propagate\n",
    "        loss.backward()\n",
    "\n",
    "        # perform the optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #add this batch's loss to the losses for this epoch\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f'Epoch {epoch}: loss {np.mean(losses)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I57busefuIRc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Evaluating the trained model\n",
    "\n",
    "1. Complete the following code to evaluate the model on the `tweet_eval` validation set. We are looping over batches (subsets) of the validation data using our validation loader, getting the most-probable category for each prediction, and adding them to a list. Then we compare them to the true categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ob441u7vuIRd",
    "outputId": "4ee49d0a-9f24-4825-df26-6dc299a2ed06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.558\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "    for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "        outputs = model(inputs) # Compute model outputs\n",
    "        # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "        vals, indices = torch.max(outputs, 1)\n",
    "        # accumulating the predictions\n",
    "        predictions += indices.tolist()\n",
    "\n",
    "# compute accuracy with sklearn accuracy_score using predictions and val_labels\n",
    "acc =accuracy_score(predictions, val_labels)\n",
    "print(f'Model accuracy: {acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When we train a model on an unbalanced dataset, it sometimes does not learn enough in order to predict the less-represented categories. Instead, it will (almost) always guess one of the larger classes, as this is a \"safer bet\" in the absence of enough informaiton. Check if in our case, the model managed to produce predictions of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WDjd9NP0uIRe",
    "outputId": "788bff32-9a89-4cfb-cd58-6e54345cd691"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether the model is able to predict all three classes\n",
    "set(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r9u6F4mDuIRf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Optional: a taste of deep neural nets\n",
    "\n",
    "Next week in lecture, we will see that deep neural nets are nothing more than neural nets with more than one layer. The layers are stacked on top of each other, so that the outputs from the first layer don't go into a prediction, until the last layer makes a prediction. Each layer first combines the outputs from the previous layer (a.k.a. activations) linearly using weights, and then applies an activation function.\n",
    "\n",
    "The neural net we are implementing here looks like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aOodcSfxuIRg",
    "tags": []
   },
   "source": [
    "<img src=\"Net for DLI exercise.drawio.png\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qVK9VZVquIRh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Finish the definition of this model. The `__init__` method should contain:\n",
    "- one hidden layer with sigmoid activation functions (this is the \"hidden\" layer of the model, between inputs and outputs, and the number of nodes in it is a variable `hidden_size` that we should be able to set when we initialize the model)\n",
    "- one softmax output layer.\n",
    "2. In the forward pass, the model should do the following:\n",
    " - compute the z values for the hidden layer (linear combinations of the inputs, using the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) neural net building block)\n",
    " - pass them through the [sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation function\n",
    " - compute the z values for the output layer (linear combinations of the hidden layer outputs, a.k.a. its activations)\n",
    " - pass them through the Softmax activation function\n",
    "3. Try initializing and inspecting the model as a toy_model instance. It should have 4 features, hidden_size 8, and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G-8FJMPduIRh",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNN(torch.nn.Module):\n",
    "\n",
    "    # initializing the model with a certain number of input features\n",
    "    # output classes, and size of hidden layer\n",
    "    def __init__(self, n_features, hidden_size, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # creating one hidden layer h1\n",
    "        # that first applies a linear transformation to the incoming data: z1 = w1T * x\n",
    "        self.h1_linear = torch.nn.Linear(n_features, hidden_size)\n",
    "\n",
    "        # creating the sigmoid activation function for the hidden layer\n",
    "        self.h1_activ = torch.nn.Sigmoid()\n",
    "\n",
    "        # creating one output layer\n",
    "        # that again applies a linear transformation to the incoming activations: z2 = w2T * a1\n",
    "        self.output_linear = torch.nn.Linear(hidden_size,n_classes)\n",
    "        \n",
    "        # creating a softmax activation function for the output layer\n",
    "        self.output_activ = torch.nn.Sigmoid()\n",
    "\n",
    "    # you have to define the forward() method which will specify the forward propagation:\n",
    "    # how the input values get to the next layer(s)\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # compute the z-values of the hidden layer\n",
    "        z1 =self.h1_linear(inputs)\n",
    "\n",
    "        # pass them through the sigmoid activation function\n",
    "        a1 = self.h1_activ(z1)\n",
    "\n",
    "        #compute the z-values of the output layer\n",
    "        z2 = self.output_linear(a1)\n",
    "        \n",
    "        # get the final values\n",
    "        outputs =self.output_activ(z2)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhwT1XM7uIRj",
    "outputId": "449a981a-de7c-4f4a-c761-e1d81fd708c3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definition of the toy_model is: \n",
      " \n",
      " SimpleNN(\n",
      "  (h1_linear): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (h1_activ): Sigmoid()\n",
      "  (output_linear): Linear(in_features=8, out_features=3, bias=True)\n",
      "  (output_activ): Sigmoid()\n",
      ")\n",
      "\n",
      "The parameters of the model: \n",
      "\n",
      "The parameter: h1_linear.weight has shape: torch.Size([8, 4])\n",
      "\n",
      "The parameter: h1_linear.bias has shape: torch.Size([8])\n",
      "\n",
      "The parameter: output_linear.weight has shape: torch.Size([3, 8])\n",
      "\n",
      "The parameter: output_linear.bias has shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Initiating model\n",
    "toymodel = SimpleNN(4,8,3)\n",
    "\n",
    "# Inspecting the toy_model\n",
    "print('The definition of the toy_model is: \\n \\n',toymodel) # Printing the definition of the model - note that there is a bias in the linear layer\n",
    "\n",
    "toy_model_dict=toymodel.state_dict() # creating a dictionary containing names and values of model parameters\n",
    "\n",
    "print('\\nThe parameters of the model: ')\n",
    "for param in toy_model_dict.keys():\n",
    "    print('\\nThe parameter: {} has shape: {}'.format(param,toy_model_dict[param].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjWS7klTuIRk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
