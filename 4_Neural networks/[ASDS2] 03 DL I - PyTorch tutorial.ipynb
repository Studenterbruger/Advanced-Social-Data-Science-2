{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# settings for tutorial presentation with RISE\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnotebook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfigManager\n\u001b[0;32m      3\u001b[0m cm \u001b[39m=\u001b[39m ConfigManager()\n\u001b[0;32m      4\u001b[0m cm\u001b[39m.\u001b[39mupdate(\u001b[39m'\u001b[39m\u001b[39mlivereveal\u001b[39m\u001b[39m'\u001b[39m, {\n\u001b[0;32m      5\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mscroll\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m               \u001b[39m'\u001b[39m\u001b[39menable_chalkboard\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m })\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'notebook'"
     ]
    }
   ],
   "source": [
    "# settings for tutorial presentation with RISE\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'width': '100%',\n",
    "              'height': '100%',\n",
    "              'scroll': True,\n",
    "              'enable_chalkboard': True,\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ASDS 2\n",
    "\n",
    "## Tutorial: PyTorch basics\n",
    "\n",
    "Clara Vandeweerdt (based on work by Anna Rogers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why PyTorch?\n",
    "\n",
    "* TensorFlow (Google)   \n",
    "* PyTorch (Facebook)  \n",
    "* Keras (François Chollet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alternative text](figures/num_hf_models_2023.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alternative text](figures/Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# package setup\n",
    "#!pip install sklearn numpy torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asger\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\advanced_social_data-1nT6mJ2B-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# enabling inline plots in Jupyter\n",
    "%matplotlib inline\n",
    "# disabling verbose messages from dataset library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just like NumPy, PyTorch provides basic functions for creating tensors and common operations on them. Tensors (in this context) are just n-dimensional arrays of numbers. So they can contain vectors, matrices and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# creating a tensor with numbers 1--5\n",
    "a = torch.FloatTensor([1,2,3,4,5])\n",
    "# same but in descending order\n",
    "b = torch.FloatTensor([5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 4., 3., 2., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6., 6.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic operations mostly work like in numpy\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type before conversion: <class 'numpy.ndarray'>\n",
      "type after conversion: <class 'torch.Tensor'>\n",
      "tensor([1, 2, 3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#we can also convert numpy arrays to torch tensors\n",
    "c = np.array([1, 2, 3, 4])\n",
    "print(\"type before conversion:\", type(c))\n",
    "c = torch.from_numpy(c)\n",
    "print(\"type after conversion:\", type(c))\n",
    "print(c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Loading the tweet_eval classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 9.72k/9.72k [00:00<00:00, 9.70MB/s]\n",
      "Downloading metadata: 100%|██████████| 30.4k/30.4k [00:00<00:00, 3.87MB/s]\n",
      "Downloading readme: 100%|██████████| 21.9k/21.9k [00:00<00:00, 3.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tweet_eval/sentiment to C:/Users/asger/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 4.97MB [00:00, 41.8MB/s]/6 [00:00<?, ?it/s]\n",
      "Downloading data: 91.2kB [00:00, 45.5MB/s]                   .37s/it]\n",
      "Downloading data: 1.16MB [00:00, 18.4MB/s]                  1.24it/s]\n",
      "Downloading data: 24.6kB [00:00, 24.5MB/s]                   .46it/s]\n",
      "Downloading data: 219kB [00:00, 10.7MB/s]                    .80it/s]\n",
      "Downloading data: 4.00kB [00:00, 3.99MB/s]                  1.99it/s]\n",
      "Downloading data files: 100%|██████████| 6/6 [00:03<00:00,  1.70it/s]\n",
      "Extracting data files: 100%|██████████| 6/6 [00:00<00:00, 118.49it/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tweet_eval downloaded and prepared to C:/Users/asger/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# load the 3-class sentiment classification model from tweet_eval\n",
    "train = load_dataset('tweet_eval', 'sentiment', split='train')\n",
    "val = load_dataset('tweet_eval', 'sentiment', split='validation')\n",
    "test = load_dataset('tweet_eval', 'sentiment', split='test')\n",
    "\n",
    "# vectorizing the data with TF-IDF corpus\n",
    "vectorizer = TfidfVectorizer() # the default ngram range is (1,1)\n",
    "\n",
    "train_corpus = [x[\"text\"] for x in train][:10000]\n",
    "train_labels = np.array([x[\"label\"] for x in train][:10000])\n",
    "train_features = vectorizer.fit_transform(train_corpus).toarray()\n",
    "\n",
    "val_corpus = [x[\"text\"] for x in val]\n",
    "val_labels = np.array([x[\"label\"] for x in val])\n",
    "val_features = vectorizer.transform(val_corpus).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converting tweet_eval data to torch tensors\n",
    "\n",
    "In this exercise, we will be working with a new HuggingFace dataset: `tweet_eval`. It contains tweets, hand-classified into three categories: negative, neutral, and positive. After loading the dataset (not shown), and turning the tweets into TF-IDF features, we need to convert the features and labels to torch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# torch.from_numpy turns these from np.arrays into torch.tensors\n",
    "x_train = torch.from_numpy(train_features).float()\n",
    "y_train = torch.from_numpy(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#same for the test data\n",
    "x_test = torch.from_numpy(val_features).float()\n",
    "y_test = torch.from_numpy(val_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Datasets and a recap of classes\n",
    "\n",
    "In PyTorch, we normally put data into objects of the [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset) class. When we create PyTorch models, those models will also be classes. That's why we could use a recap of classes in Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classes in Python\n",
    "\n",
    "Almost everything in Python is an object. Objects have classes that tell us what kind of objects they are. The class of an object determines what *attributes* it can have (for example, an object of class Pet can have its species as an attribute), and what *methods* we can apply to it (what it can do; for example, we can ask any Pet to tell us what species it is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodle\n"
     ]
    }
   ],
   "source": [
    "class Pet:\n",
    " \n",
    "    # What happens when we create an instance of Pet\n",
    "    def __init__(self, pet_species):\n",
    "        self.species = pet_species\n",
    " \n",
    "    # A method for a Pet (something it can do)\n",
    "    def whatareyou(self):\n",
    "        print(\"I'm a\", self.species)\n",
    " \n",
    " \n",
    "# Object instantiation\n",
    "Rodger = Pet(\"poodle\")\n",
    "\n",
    "# Using class attributes and methods\n",
    "print(Rodger.species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a poodle\n"
     ]
    }
   ],
   "source": [
    "Rodger.whatareyou()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also create subclasses. Subclasses have all the same attributes and methods as their parent class, and then we can modify them or add some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodle\n",
      "I'm a poodle\n",
      "woof woof\n"
     ]
    }
   ],
   "source": [
    "class Dog(Pet):\n",
    "    \n",
    "    # A method for a Dog (something it can do)\n",
    "    def bark(self):\n",
    "        print(\"woof woof\")\n",
    "\n",
    "# Object instantiation\n",
    "Jazz = Dog(\"poodle\")\n",
    "\n",
    "# Using class attributes and methods\n",
    "print(Jazz.species)\n",
    "Jazz.whatareyou()\n",
    "Jazz.bark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Dataset Class\n",
    "\n",
    "We are going to define our own subclass `TweetEvalData` that is a special case of the `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Making a subclass of the torch Datasets class\n",
    "\n",
    "class TweetEvalData(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y): # describes how the dataset is initialized; the arguments (when initializing) are the features and labels\n",
    "        self.X = X #any instance of the TweetEvalData class will have an attribute X that contains its features \n",
    "        self.y = y #same with an attribute y that contains its labels\n",
    "\n",
    "    def __getitem__(self, index): # getitem allows to retrieve a datapoint from the dataset by its index.\n",
    "        X = self.X[index] \n",
    "        y = self.y[index].unsqueeze(0) #tensor is unsqueezed to ensure correct shape for training\n",
    "        return X, y # methods returns corresponding data point to input index by an x and y tensor \n",
    "\n",
    "    # a helper to check and return the size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.y) # Returning the number of labels in the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`TweetEvalData` is a class, so to use it you need to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing datasets\n",
    "\n",
    "dataset_class_train = TweetEvalData(x_train, y_train) # initiating an instance of the class that contains the train data\n",
    "dataset_class_val = TweetEvalData(x_test, y_test) #initating an instance of the class that contains the test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We also need so-called `Dataloaders` for each of these data sets. A `Dataloader` allows us to easily iterate over samples of data and corresponding labels during training and evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initiating dataloader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_class_train, batch_size = 64)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_class_val, batch_size = 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Batch size is the number of samples that are processed before the model is updated while training; each step of our gradient descent is based on one batch of data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Defining and training a model in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic model architecture: a linear classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All the models we will implement in PyTorch will be subclasses of the existing [`torch.nn.Module` class](https://pytorch.org/docs/stable/nn.html?highlight=module#torch.nn.Module). In the `__init__` method of your model should define all the layers you are going to use. The `forward()` method defines the order of the layers, and so, how the model should produce outputs given the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define a class for a linear classifier\n",
    "class LinearClassifier(torch.nn.Module):\n",
    "    # initialization parameters\n",
    "    def __init__ (self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        # we will have only one linear layer which takes the given number of features as its inputs,\n",
    "        # and outputs a score for each of the given number of classes\n",
    "        self.linear = torch.nn.Linear(n_features, n_classes)\n",
    "\n",
    "    # you always need to define the forward() method which defines how your model performs\n",
    "    # forward propagation\n",
    "    def forward(self, x):\n",
    "        linear_out = self.linear(x)\n",
    "        return linear_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting things up for training\n",
    "\n",
    "1) instantiating the model\n",
    "2) choosing a loss function (e.g. [Mean Squared Error loss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss) or [Cross Entropy loss](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)) and computing the loss\n",
    "3) choosing an optimizer--we will be working with the Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the model with the input size equal to the number of features in the data\n",
    "myLC = LinearClassifier(n_features=18484, n_classes=3)\n",
    "\n",
    "# setting up the loss function component \n",
    "# which will implicitly perform softmax on linear layer outputs\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# setting up the optimizer (stochastic gradient descent)\n",
    "optimizer = torch.optim.Adam(myLC.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The training loop\n",
    "\n",
    "For every batch (subset of data points):\n",
    "\n",
    "1) feed the inputs to the model to get predictions\n",
    "\n",
    "2) compare to targets (labels) to compute the loss\n",
    "\n",
    "3) compute the gradients based on this loss using `loss.backward()`. This is where PyTorch does magic with automatic differentiation\n",
    "\n",
    "4) update the weights of the model using `optimizer.step()`\n",
    "\n",
    "Repeat (go over the whole dataset in batches again) for as many \"epochs\" as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0563572432584822\n",
      "0.9913962517574335\n",
      "0.9515947593245536\n",
      "0.9203479327973286\n",
      "0.8927042005927699\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    for batch_index, (inputs, targets) in enumerate(train_loader):    \n",
    "    \n",
    "        #backward function accumulates gradients by default\n",
    "        # use .zero_grad() to start from scratch each time\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = myLC(inputs)\n",
    "        # Compute Loss, given the true labels for the training data\n",
    "        targets = torch.flatten(targets)\n",
    "        targets = targets.type(torch.LongTensor) # Converting targets as required for loss function\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        # performs a parameter update based on the current gradient (stored in .grad attribute of a parameter)\n",
    "        # and the update rule\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keeping track of the loss values\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluating the trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once the model is trained, we can get predictions for the test set and check accuracy. A key difference with training is that the backward pass is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.578\n"
     ]
    }
   ],
   "source": [
    "# disabling gradient updates \n",
    "# this will reduce memory consumption, and is a good practice at inference time\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "    \n",
    "        # making predictions on the test set and evaluating the model\n",
    "        outputs = myLC(inputs)\n",
    "    \n",
    "        # now we want accuracy, not loss. So we need an actual prediction:\n",
    "        # we will predict the class with the highest score\n",
    "        vals, indices = torch.max(outputs, 1)\n",
    "        predictions += indices.tolist()\n",
    "\n",
    "acc = accuracy_score(predictions, val_labels)\n",
    "print(f'Model accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
