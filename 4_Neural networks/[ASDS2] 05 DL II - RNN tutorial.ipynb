{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': '100%', 'height': '100%', 'scroll': True, 'enable_chalkboard': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settings for tutorial presentation with RISE\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'width': '100%',\n",
    "              'height': '100%',\n",
    "              'scroll': True,\n",
    "              'enable_chalkboard': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ASDS 2\n",
    "\n",
    "## Tutorial: Recurrent Neural Nets with PyTorch\n",
    "\n",
    "Anna Rogers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: basic neural network in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    # initialization parameters\n",
    "    def __init__ (self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        # we will have only one linear layer which takes the given number of features as its inputs,\n",
    "        # and outputs a score for each of the given number of classes\n",
    "        self.linear = torch.nn.Linear(n_features, n_classes)\n",
    "\n",
    "    # you always need to define the forward() method which defines how your model performs\n",
    "    # forward propagation\n",
    "    def forward(self, x):\n",
    "        linear_out = self.linear(x)\n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN in PyTorch\n",
    "\n",
    "Introducing a new layer type: the [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layer. The cells in the RNN layer have `input_size` and `hidden_size` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rnn_layer = torch.nn.RNN(input_size=5, hidden_size=2, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`batch_first` means that we will be feeding it multidimensional tensors where the first dimension is the batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The input of the RNN layer has three dimensions: the number of observations (sequences) in the batch, the number of inputs in one sequence, and the number of features per input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sample_input = torch.rand(3, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The output of the RNN layer is a tuple of (all_hidden_states, final_hidden_state). There are as many hidden states as tokens in the input sequence, because each RNN cell takes in one input token and produces one hidden state. The final hidden state is just the hidden state of only the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "all_hidden_states, last_hidden_state = rnn_layer(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5234, 0.2906],\n",
      "         [0.4931, 0.7405],\n",
      "         [0.9336, 0.8856],\n",
      "         [0.8287, 0.6910]],\n",
      "\n",
      "        [[0.1451, 0.5003],\n",
      "         [0.5696, 0.7911],\n",
      "         [0.7198, 0.7579],\n",
      "         [0.7475, 0.6627]],\n",
      "\n",
      "        [[0.2829, 0.5905],\n",
      "         [0.8499, 0.8172],\n",
      "         [0.8443, 0.7516],\n",
      "         [0.6159, 0.8172]]], grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# we have a batch of 3 sequences; each sequence leads to 4 hidden states (~ input tokens),\n",
    "# and we told the model to produce 2-dimensional hidden states. So we get 3x4x2 numbers.\n",
    "print(all_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8287, 0.6910],\n",
      "         [0.7475, 0.6627],\n",
      "         [0.6159, 0.8172]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# last_hidden_state just reproduces the final hidden state from each batch. So 3x2 numbers.\n",
    "print(last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using the RNN activations\n",
    "\n",
    "In the exercise we will once again try to classify tweets into three sentiment categories. This is a many-to-one set-up. We will therefore only use the final hidden state of the RNN. We can then feed it into an output layer to produce our final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3221, 0.3350, 0.3363],\n",
       "         [0.2947, 0.3695, 0.3500],\n",
       "         [0.3833, 0.2955, 0.3137]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear combination of the (in this case two) elements of the hidden states\n",
    "# output three scores: one for each of the options in our classification problem \n",
    "linear_layer = torch.nn.Linear(2, 3)\n",
    "# softmax activation function - normalizing output to have sum=1\n",
    "activ_layer = torch.nn.Softmax(dim=1)\n",
    "\n",
    "# produce some example output. Note we are only using the last hidden state\n",
    "linear = linear_layer(last_hidden_state)\n",
    "activ = activ_layer(linear)\n",
    "\n",
    "# for each of the 3 sequences in our example batch,\n",
    "# we get probabilities for the 3 classes \n",
    "activ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Note:** in the exercise set, we will not be using a softmax layer. We will only be doing a linear step in the output layer.\n",
    "\n",
    "The reason is that when we later calculate the cross-entropy loss with PyTorch's built-in [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) function, that function applies a softmax transformation for us (before calculating the loss).\n",
    "\n",
    "So, we are \"outsourcing\" the final softmax activation function of the model to the loss calculation. (we actually did this wrong in PSet 4.1 and added a softmax layer where none was needed--will fix!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
