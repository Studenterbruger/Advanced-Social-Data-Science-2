{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install gensim torch numpy pandas nltk sklearn matplotlib datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "# enabling inline plots in Jupyter\n",
    "%matplotlib inline\n",
    "datasets.logging.set_verbosity_error()\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: RNNs\n",
    "\n",
    "In this exercise session, you will be building on your knowledge from the previous session to define and train an RNN in PyTorch. This will be a many-to-one RNN, which takes in tweets as sequences of words and outputs a classification of the tweet as negative, neutral or positive. First, we will load the `tweet_eval` dataset, convert the words into the tweets into embeddings (more on this below) and turn it into a Torch dataset. Next, we will set up the RNN. Then, we will train and evaluate our network on the data. Finally, we will explore alternatives to simple RNNs: LSTMs, bidirectional RNNs, and deep RNNs.\n",
    "\n",
    "We will provide you with code snippets to help you get started. Where you see a `FILLINTHEBLANK` in the code, or a line that ends in a `=`, that is where you complete the code to make it functional.\n",
    "\n",
    "Note: in order to see the embedding matrix figure included in Problem 2, you need to download the file `embedding_matrix.png` from Absalon and store it in the same folder as this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Finish the FNN exercise.\n",
    "\n",
    "If you haven't finished the previous exercise set (softmax regression with TF-IDF vectors as a neural net), please finish these before continuing with the RNN exercise. Refer to the previous exercise and solutions notebooks as needed. You don't need to have finished the final, optional step in the previous problem set (\"a taste of deep neural networks\") in order to start this problem set, but if you finish early with the current problem set, then you can return to that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Prepare the tweet_eval dataset\n",
    "\n",
    "1. Load the train and validation data with the huggingface data loader. If your computer is struggling, use a subset of the training data.\n",
    "2. Build a full list of the vocabulary (all possible tokens) in the training and validation data. As you see in the skeleton code below, we add an extra empty token \"\" at the beginning of the dictionary. This is because we ultimately want all inputs (tweets in this case) to be the same length (have the same word count). To do that, we will later \"pad\" each tweet at the beginning with this padding token, until it is as long as the longest tweet. E.g. if the longest tweet were 10 words, we would tokenize a tweet saying \"the cat is on the mat\" as: \"\", \"\", \"\", \"\", \"the\", \"cat\", is\", \"on\", \"the\", \"mat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the 3-class sentiment classification data from tweet_eval\n",
    "train = datasets.load_dataset('tweet_eval', 'sentiment', split='train')\n",
    "val = datasets.load_dataset('tweet_eval', 'sentiment', split='validation')\n",
    "\n",
    "train_corpus = [x[\"text\"] for x in train][:10000]\n",
    "train_labels = [x[\"label\"] for x in train][:10000]\n",
    "\n",
    "val_corpus = [x[\"text\"] for x in val]\n",
    "val_labels = [x[\"label\"] for x in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23106"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the full list of vocabulary in the tweet_eval data\n",
    "total_vocabulary = set()\n",
    "for tweet in train_corpus + val_corpus:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    for t in tokens:\n",
    "        total_vocabulary.add(t.lower())\n",
    "total_vocabulary = sorted(list(total_vocabulary))\n",
    "# appending an empty token to 'save' the zero position for the padding token, and shift all the indices\n",
    "total_vocabulary = [\"\"]+total_vocabulary\n",
    "\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this exercise, instead of using TF-IDF as input, we will use all the words of the tweets in their original order. The way we will feed these words into the RNN in the form of numbers is by \"embedding\" them first. We will talk much more about embeddings later, but what you need to know right now is that embeddings translate tokens like \"dog\" into vectors (lists of numbers) that capture the underlying meanings of the words. We will use pre-trained embeddings, which means that the way to convert words to vectors was learned by another model on another data set (in this case, still a Twitter data set).\n",
    "\n",
    "In practice, we do this by giving the model a so-called embedding matrix, with one row per vocabulary word, where the row is the vector that embeds the meaning of that word (see figure below). The code below builds an embedding matrix given a vocabulary using the Gensim library's `glove-twitter-200` downloadable embedding (or `glove-twitter-25` if you find your computer is struggling). It also gives an all-zero embedding to the padding token. Finally, since we are using pre-trained embeddings we might have words in our data that are not present in the embedding. These are called out-of-vocabulary (oov) words, and we will assign an all-zero embedding to those as well.\n",
    "\n",
    "![image.png](embedding_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.79875357050117 % of tokens are out of vocabulary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_embedding_matrix(tokens, embedding):\n",
    "    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n",
    "    vector of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n",
    "    oov = set()\n",
    "    size = embedding.vector_size\n",
    "    # note the extra zero vector that will used for padding\n",
    "    embedding_matrix=np.zeros((len(tokens),size))\n",
    "    c = 0\n",
    "    for i in range(1,len(tokens)):\n",
    "        try:\n",
    "            embedding_matrix[i]=embedding[tokens[i]]\n",
    "        except KeyError: #to catch the words missing in the embeddings\n",
    "            try:\n",
    "                embedding_matrix[i]=embedding[tokens[i].lower()]\n",
    "            except KeyError:\n",
    "                #if the token does not have an embedding, we initialize it as a vector of 0s\n",
    "                embedding_matrix[i] = np.zeros(size)\n",
    "                #we keep track of the out of vocabulary tokens\n",
    "                oov.add(tokens[i])\n",
    "                c +=1\n",
    "    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')\n",
    "    return embedding_matrix, oov\n",
    "\n",
    "# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix\n",
    "glove = gensim.downloader.load('glove-twitter-200')\n",
    "\n",
    "#get the embedding matrix for our tweet_eval vocabulary\n",
    "embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Convert the tweets to vectors of indices showing which words they contain (these indices will also correspond to the rows of those words in the embedding matrix), including their padding. For example, \"the cat is on the mat\" could be represented as: [ 0, 0, 0, 0, 5, 1, 2, 4, 5, 3], if those were the positions that corresponded to those words in our dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# you do not need to modify this code block; simply read through the comments, so you understand what it does, and then run it\n",
    "\n",
    "def text_to_indices(text, total_vocabulary):\n",
    "    \"\"\"turns the input text (one tweet) into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n",
    "    encoded_text = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            index = total_vocabulary.index(t.lower())\n",
    "            encoded_text.append(index)\n",
    "        except:\n",
    "            continue\n",
    "    return encoded_text\n",
    "\n",
    "def add_padding(vector, max_length, padding_index):\n",
    "    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length (the length of tweet with most words)\"\"\"\n",
    "    if len(vector) < max_length:\n",
    "        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting the feature vectors by applying the text_to_indices function to each\n",
    "# tweet in the train and validation corpus\n",
    "train_features = [text_to_indices(x, total_vocabulary) for x in train_corpus]\n",
    "val_features = [text_to_indices(x, total_vocabulary) for x in val_corpus]\n",
    "\n",
    "longest_tweet = max(train_features+val_features, key=len)\n",
    "max_length = len(longest_tweet)\n",
    "padding_index = 0\n",
    "\n",
    "# padding the feature vectors by applying the add_padding function to each\n",
    "# tweet in the train and validation corpus\n",
    "train_features = [add_padding(x, max_length, padding_index) for x in train_features]\n",
    "val_features = [add_padding(x, max_length, padding_index) for x in val_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pass the training and validation data to our custom PyTorch DataSet class, and pass that to a data loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetEvalTrain(torch.utils.data.Dataset):\n",
    "    # defining the sources of the data\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.LongTensor(features)\n",
    "        self.y = torch.from_numpy(np.array(labels)).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "data_train = TweetEvalTrain(train_features, train_labels)\n",
    "data_val = TweetEvalTrain(val_features, val_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(data_val, batch_size = 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Define and train the RNN model\n",
    "\n",
    "1. Use the skeleton code below to create an RNN model with pre-trained embedding layer. The embedding layer should be followed by a [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) layer, which in its turn is followed by a linear layer. In the forward pass, the model should encode the input (turn it into embeddings), retrieve the final activation state from the last RNN unit. The activation of the final unit in an RNN will (hopefully) encode all relevant info about the whole sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# defining the simple RNN model + embedding layer\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "\n",
    "         #define the RNN itself\n",
    "        self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, batch_first=True) #set batch_first=True your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.fc_logits = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "\n",
    "        # The RNN returns two tensors: one representing the outputs at all positions,\n",
    "        # and another representing only the final activation states.\n",
    "        # In this many-to-one model, we only need the final states.\n",
    "        all_states, final_state = self.rnn(encoded_inputs)\n",
    "        final_state = final_state.squeeze()\n",
    "\n",
    "\n",
    "        # run the output through the final linear layer\n",
    "        #we squeeze the final state to get rid of the extra dimension\n",
    "        outputslinear = self.fc_logits(final_state)\n",
    "        return outputslinear\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instatiate a version of the model with `rnn_size=100` and inspect it to see its layers and dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(23106, 200, padding_idx=0)\n",
      "  (rnn): RNN(200, 100, batch_first=True)\n",
      "  (fc_logits): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "print(myRNN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try training the model with the same Adam gradient descent optimizer and CrossEntropyLoss as in the previous problem set, training for 3 epochs. Then evaluate its accuracy.\n",
    "\n",
    "`Note`. We did not use the softmax layer in the forward function. This is because CrossEntropyLoss applies softmax automatically and therefore the function expects the unnormalized probabilities from the final linear layer. That means, however, that your evaluate function needs to use the softmax function on the outputs of your model when predicting. The following snippet might be helpful for evaluation\n",
    "\n",
    "torch.softmax(model(inputs), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.squeeze() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc}')\n",
    "    return acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.9377114556397602\n",
      "Epoch 2: loss 0.8323918022927205\n",
      "Epoch 3: loss 0.8243292638450671\n",
      "Model accuracy: 0.583\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, 3)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Other types of RNNs\n",
    "\n",
    "1. Implement a `type` parameter for the model which will control the type of RNN cell used: [vanilla RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) or [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). Note that the output from LSTM cells is different from RNN cells, and we need to adjust our output handling in the forward pass accordingly. Try training the LSTM version of the model for 3 epochs, and compare its accuracy to the RNN version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced version supporting multiple types of RNN layers\n",
    "\n",
    "class RNN_or_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, type=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        if type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, batch_first=True)\n",
    "        elif type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, bidirectional=False, num_layers=1, batch_first=True)\n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "            \n",
    "        self.fc_logits = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        #apply the RNN or LSTM\n",
    "        if type == \"RNN\":\n",
    "            rnn_out, final_state = self.rnn(encoded_inputs)\n",
    "        else:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            rnn_out, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "        \n",
    "        # run the final states through the output layer\n",
    "        outputslinear = self.fc_logits(final_state.squeeze())\n",
    "        return outputslinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.9032310676422848\n",
      "Epoch 2: loss 0.7808112164211881\n",
      "Epoch 3: loss 0.7310783709310422\n",
      "Model accuracy: 0.6485\n"
     ]
    }
   ],
   "source": [
    "myLSTM = RNN_or_LSTM(rnn_size=100, n_classes=3, type = 'LSTM', embedding_matrix=embedding_matrix)\n",
    "\n",
    "myLSTM = training_loop(myLSTM, 3)\n",
    "acc, preds = evaluate(myLSTM, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Also implement a bidirectional RNN instead of a standard, one-direction RNN. You will have to concatenate the final state of the left-to-right RNN with that of the right-to-left RNN, and feed that into your output layer. Use the code snippet below in your model definition to help you use right parts of the final state tensors. Also that your linear layer has the correct input size--its inputs are now twice the size of the RNN unit activations, because it is getting activations from two different RNNs. Train and evaluate this model as before.\n",
    "\n",
    "Note: you will see that the bidirectional RNN does not get us better performance. This has something to do with the way we are front-padding the tweets (to equalize their length), combined with the fact that the BRNN goes through the tweets in reverse, so right to left. Thinking of the vanishing gradient problem, can you guess why the second, reverse RNN does not help us in this case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bidirectional_RNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # because we are using the bidirectional RNN we will concatenate the tensor from the first\n",
    "        # and last token in our sequence. This means the output of our RNN will be twice\n",
    "        # the size of the hidden size and we should make the neccessary adjustments for the linear layer.\n",
    "        self.fc_logits = torch.nn.Linear(2*rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        # NB: for a bidirectional RNN, the final state corresponds to the *last* token\n",
    "        # in the forward direction and the *first* token in the backward direction.\n",
    "        #Notice that we use torch.concat to concatenate the final states from the forward and backward directions\n",
    "        rnn_out, final_state = self.rnn(encoded_inputs)\n",
    "        final_states_combined = torch.cat([final_state[-2,:,:], final_state[-1,:,:]], dim=1)\n",
    "\n",
    "        # run the output through the final linear layer\n",
    "        outputslinear = self.fc_logits(final_states_combined)\n",
    "        return outputslinear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.9373165870168406\n",
      "Epoch 2: loss 0.861334123049572\n",
      "Epoch 3: loss 0.8148881827190424\n",
      "Model accuracy: 0.619\n"
     ]
    }
   ],
   "source": [
    "biRNN = Bidirectional_RNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "biRNN = training_loop(biRNN, 3)\n",
    "acc, preds = evaluate(biRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BRNN does not improve performance, because all of the actual word tokens relevant to the sentiment prediction are at the end of the sequence, after the front-padding with \"\". The right-to-left RNN will encounter this relevant information first (in its forward pass), and then will have to deal with a bunch of padding tokens before getting to its final state. The vanishing gradient problem means that the model will struggle to learn proper weights for the nodes at the end of the sequence, because they are too far removed from the final state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Experiment with a second recurrent layer to implement a deep (or stacked) RNN. This can be done using the parameters of [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the simple RNN model + embedding layer\n",
    "\n",
    "class DeepRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, num_layers):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "\n",
    "        #define the RNN itself and a final softmax output layer\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_logits = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "\n",
    "        # The RNN returns two tensors: one representing the outputs at all positions,\n",
    "        # and another representing only the final activation states.\n",
    "        # In this many-to-one model, we only need the final states.\n",
    "        rnn_out, final_state = self.rnn(encoded_inputs)\n",
    "\n",
    "        # we only need the activations from the last rnn layer (i.e. the last layer in the stack of layers)\n",
    "        outputslinear = self.fc_logits(final_state[-1,:,:])\n",
    "        return outputslinear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.9213439666541519\n",
      "Epoch 2: loss 0.8474376535719368\n",
      "Epoch 3: loss 0.81326250627542\n",
      "Model accuracy: 0.614\n"
     ]
    }
   ],
   "source": [
    "deRNN = DeepRNN(rnn_size=100, n_classes=3, num_layers=2, embedding_matrix=embedding_matrix)\n",
    "\n",
    "deRNN = training_loop(deRNN, 3)\n",
    "acc, preds = evaluate(deRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep RNN also leads to very minor performance gains at best. Deeper nets are not always better! It all depends on how much data we have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
