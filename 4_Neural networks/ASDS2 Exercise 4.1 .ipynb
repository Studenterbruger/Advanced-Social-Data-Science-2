{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asger\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\advanced_social_data-1nT6mJ2B-py3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import scipy\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Shallow neural networks\n",
    "\n",
    "In this exercise session, you will be defining and training a multiclass logistic regression model (a.k.a. softmax regression or multinomial logit) as a simple neural net in PyTorch. First, we will set up the model. Then, we will load the `tweet_eval` dataset and turn it into a Torch dataset (the type of data input that PyTorch can work with). Next, we will train our network on the data using a variant of gradient descent called Adam. Finally, we will evaluate the model's performance.\n",
    "\n",
    "We will provide you with code snippets to help you get started. Where you see a `FILLINTHEBLANK` in the code, or a line that ends in a `=`, that is where you complete the code to make it functional.\n",
    "\n",
    "Note: in order to have this notebook display the Figure in problem 5, you should also download the file `Net for DLI exercise.drawio.png` from Absalon and store it in the same folder as this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Define the PyTorch model\n",
    "\n",
    "Finish the definition of this model.\n",
    "\n",
    "1. In the `__init__` method of your model you should define all the layers you are going to use. PyTorch provides a large amount of commonly used layers that are very easy to use. Please refer to the [documentation of PyTorch](https://pytorch.org/docs/stable/nn.html) for a complete list of layers. Here, the `__init__` method should contain one layer that linearly combines the inputs, and then a softmax activation function, which will produce the outputs.\n",
    "2. In the forward pass, the model should do the following:\n",
    " - compute the z values of the node (linear combinations of the inputs, using the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) neural net building block). There should be as many outputs as classes here (since the probability of each class gets to be based on its own linear combination of the features)\n",
    " - pass them through the [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) activation function with `dim=1` to normalize them (squeeze the probabilities of all output classes so that they sum to one)\n",
    " - return the outputs\n",
    "3. Try initializing and inspecting the model as a toy_model instance. It should have 4 features and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise template\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "\n",
    "    # initializing the model with a certain number of input features\n",
    "    # and output classes\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # creating a layer that applies a linear transformation\n",
    "        # to the incoming data: z = wT * x\n",
    "        self.linear = torch.nn.Linear(n_features,n_classes) # Laver et lag\n",
    "\n",
    "\n",
    "        # creating a softmax activation function\n",
    "        self.activ = torch.nn.Softmax(dim=1) # Konvertere alle tal til probability distribution\n",
    "        \n",
    "    # you have to define the forward() method which will specify the forward propagation:\n",
    "    # how the input values get to the next layer(s)\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # compute the linear z-values for the layer\n",
    "        z = self.linear(inputs) # \n",
    "\n",
    "        # pass them through the softmax activation function\n",
    "        outputs = self.activ(z)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of LogisticRegression(\n",
       "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (activ): Softmax(dim=1)\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting number of input features and classes\n",
    "n_features=4\n",
    "n_classes=3\n",
    "\n",
    "toy_model = LogisticRegression(n_features, n_classes)\n",
    "toy_model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definition of the toy_model is: \n",
      " \n",
      " LogisticRegression(\n",
      "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (activ): Softmax(dim=1)\n",
      ")\n",
      "\n",
      "The parameters of the model: \n",
      "\n",
      "The parameter: linear.weight has shape: torch.Size([3, 4])\n",
      "\n",
      "The parameter: linear.bias has shape: torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the toy_model\n",
    "\n",
    "print('The definition of the toy_model is: \\n \\n',toy_model) # Printing the definition of the model\n",
    "\n",
    "toy_model_dict=toy_model.state_dict() # creating a dictionary containing names and values of model parameters\n",
    "\n",
    "print('\\nThe parameters of the model: ')\n",
    "for param in toy_model_dict.keys():\n",
    "    print('\\nThe parameter: {} has shape: {}'.format(param,toy_model_dict[param].shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare the *Tweet_eval* dataset\n",
    "\n",
    "This is a data set of tweets that are hand-coded as either negative, neutral or positive\n",
    "\n",
    "## Load the data set\n",
    "\n",
    "1. Have a look at the paper by Rosenthal et al. below. How were the tweets selected? How were they annotated?\n",
    "2. Load the dataset (train and validation splits) from the huggingface library.\n",
    "3. Have a look at the dataset on [huggingface's website](https://huggingface.co/datasets/tweet_eval/viewer/sentiment/train), to get a sense of what's in it. Also do a count of the number of training samples with each label, to see whether the data is balanced (all classes represented evenly) or unbalanced.\n",
    "\n",
    "The sentiment task is described in this paper:\n",
    "\n",
    "> Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 Task 4: Sentiment Analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502â€“518, Vancouver, Canada. Association for Computational Linguistics. [https://aclanthology.org/S17-2088/](https://aclanthology.org/S17-2088/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading traing and validation data \n",
    "train = datasets.load_dataset('tweet_eval', 'sentiment', split='train')\n",
    "val = datasets.load_dataset('tweet_eval', 'sentiment', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20673\n",
       "2    17849\n",
       "0     7093\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(train)\n",
    "label_counts[\"label\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Turn it to Torch tensors\n",
    "\n",
    "1. Vectorize the tweet texts with the TfIDF vectorizer from sklearn\n",
    "2. Convert this data to Torch tensors. Note that the output of the sklearn vectorizer is not numpy arrays but scipy matrices, which can be converted with `toarray()` method. Next, you can convert those to torch vectors using the torch[from_numpy](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) method. Finally, you will need to convert the feature tensors to float type with `float()` method.\n",
    "\n",
    "Labels are lists, and so can be converted to numpy arrays with `np.array(mylist)`. Then we also need to turn those into torch vectors. \n",
    "\n",
    "If your computer is struggling with the conversion, simply reduce the amount of training data to a slice (e.g. first 10K examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "train_corpus = [x[\"text\"] for x in train] # Samler alle dokumenterne i en\n",
    "train_labels = [x[\"label\"] for x in train] # Samler alle deres rigtige klassificeringer\n",
    "train_features = vectorizer.fit_transform(train_corpus) \n",
    "\n",
    "val_corpus = [x[\"text\"] for x in val]\n",
    "val_labels = [x[\"label\"] for x in val]\n",
    "val_features = vectorizer.transform(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45615, 43358]) torch.Size([2000, 43358])\n",
      "torch.Size([45615]) torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "# skeleton code for step 2, converting the vectorizer data to Torch tensors\n",
    "\n",
    "x_train = torch.from_numpy(train_features.toarray()).float()\n",
    "y_train = torch.from_numpy(np.array(train_labels))\n",
    "x_test = torch.from_numpy(val_features.toarray()).float()\n",
    "y_test = torch.from_numpy(np.array(val_labels))\n",
    "\n",
    "print(x_train.size(), x_test.size())\n",
    "print(y_train.size(), y_test.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Turn the data into Torch Datasets\n",
    "\n",
    "We're still not done with the data preparation! The canonical way to handle data in PyTorch is with objects of the [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset) class. More specifically, we are going to define our own subclass `TweetEvalData` that is a special case of the `Dataset` class. There is an official PyTorch [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "1. You are provided with the code for the class `TweetEvalData` (a subclass of the torch Dataset class) that can contain our `tweet_eval` data. Fill in the parts of the new class definition where provide the features and labels. Next, you will need to complete the code that creates two instances of that class: one for the train data and one for the validation data. The features and labels must come in the form of torch tensors. Luckily, you have just done that in the previous step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Making a subclass of the torch Datasets class\n",
    "# You don't need to modify this code; just take a look at\n",
    "# what it does and run it\n",
    "\n",
    "class TweetEvalData(torch.utils.data.Dataset): # initiating a class for representing data with three methods\n",
    "\n",
    "    def __init__(self, X, y): # describes how the dataset is initialized; the arguments (when initializing) are the features and labels\n",
    "        self.X = X #any instance of the TweetEvalData class will have an attribute X that contains its features \n",
    "        self.y = y #same with an attribute y that contains its labels\n",
    "\n",
    "    def __getitem__(self, index): # getitem allows to retrieve a datapoint from the dataset by its index.\n",
    "        X = self.X[index] \n",
    "        y = self.y[index].unsqueeze(0) #tensor is unsqueezed to ensure correct shape for training\n",
    "        return X, y # methods returns corresponding data point to input index by an x and y tensor \n",
    "\n",
    "    # a helper to check and return the size of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.y) # Returning the number of labels in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing datasets\n",
    "\n",
    "dataset_class_train = TweetEvalData(x_train, y_train) # initiating an instance of the class that contains the train data\n",
    "dataset_class_val = TweetEvalData(x_test, y_test) #initating an instance of the class that contains the test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Finally, we need so-called [Dataloaders](https://pytorch.org/docs/stable/data.html) for each of these data sets. As you will see in the example code later on, a Dataloader allows us to easily iterate over samples of data and corresponding labels during training of our model. Create Dataloaders for the train and test sets using `torch.utils.data.DataLoader`, with `batch_size` 64. Batch size is number of samples that are processed before the model is updated while training; each step of our gradient descent is based on one batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating dataloaders \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_class_train, batch_size = 64)\n",
    "val_loader = torch.utils.data.DataLoader(dataset_class_val, batch_size = 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Let's train our neural network!\n",
    "\n",
    "1. Create an instance of our LogisticRegression. The input feature size should correspond to the size of tfidf vectors. We still have 3-class classification.\n",
    "2. Set up a loss function ([CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) and an optimizer, a.k.a. a gradient descent algorithm ([Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate 0.001)\n",
    "3. Complete and run the provided code for training the model across 5 epochs. Is your loss going down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=43358, out_features=3, bias=True)\n",
       "  (activ): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(43358, 3)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss() # loss function for tracing loss during training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer to update weights and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 0.9812879066647855\n",
      "Epoch 1: loss 0.9614523442731164\n",
      "Epoch 2: loss 0.9465684226151101\n",
      "Epoch 3: loss 0.9345587840588531\n",
      "Epoch 4: loss 0.9242646756881082\n",
      "Epoch 5: loss 0.9146081698225056\n",
      "Epoch 6: loss 0.9038995164139575\n",
      "Epoch 7: loss 0.8920980046541126\n",
      "Epoch 8: loss 0.8815186363115886\n",
      "Epoch 9: loss 0.8722722455963681\n"
     ]
    }
   ],
   "source": [
    "# skeleton code for step (3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = [] # storing the loss values for this epoch\n",
    "    for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "        # zero the gradients that are stored from the previous optimization step\n",
    "        optimizer.zero_grad() # SÃ¸rger for at de gradients der er fra forrige gruppe ikke bliver brugt mere\n",
    "        \n",
    "        # get the model outputs and the original target (true) labels\n",
    "        outputs = model(inputs) # compute the outputs of the model\n",
    "        targets = torch.flatten(targets) # converting target tensor from shape (batch_size,1) to (batch_size)\n",
    "        targets = targets.type(torch.LongTensor) # Converting targets as required for loss function\n",
    "        \n",
    "        # compute the loss here\n",
    "        loss = loss_function(outputs,targets)\n",
    "\n",
    "        # back-propagate\n",
    "        loss.backward()\n",
    "\n",
    "        # perform the optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        #add this batch's loss to the losses for this epoch\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(f'Epoch {epoch}: loss {np.mean(losses)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Evaluating the trained model\n",
    "\n",
    "1. Complete the following code to evaluate the model on the `tweet_eval` validation set. We are looping over batches (subsets) of the validation data using our validation loader, getting the most-probable category for each prediction, and adding them to a list. Then we compare them to the true categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.6425\n"
     ]
    }
   ],
   "source": [
    "# skeleton code\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad(): #this is evaluation, so we don't need to do backpropagation anymore\n",
    "    for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "        outputs = model(inputs) # compute model outputs\n",
    "        # getting the indices of the output with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "        vals, indices = torch.max(outputs, 1)\n",
    "        # accumulating the predictions\n",
    "        predictions += indices.tolist()\n",
    "\n",
    "# compute accuracy on the predicted and target values with sklearn accuracy_score.\n",
    "# Use the original list of validation labels loaded from the tweet_eval dataset\n",
    "acc = accuracy_score(predictions, val_labels)\n",
    "print(f'Model accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6385, 0.5847, 0.8851, 0.7758, 0.5371, 0.4181, 0.7059, 0.5460, 0.8077,\n",
       "        0.7533, 0.9697, 0.5906, 0.4392, 0.7644, 0.6321, 0.5286])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(outputs, 1)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When we train a model on an unbalanced dataset, it sometimes does not learn enough in order to predict the less-represented categories. Instead, it will (almost) always guess one of the larger classes, as this is a \"safer bet\" in the absence of enough informaiton. Check if in our case, the model managed to produce predictions of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether the model is able to predict all three classes\n",
    "set(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Optional: a taste of deep neural nets\n",
    "\n",
    "Next week in lecture, we will see that deep neural nets are nothing more than neural nets with more than one layer. The layers are stacked on top of each other, so that the outputs from the first layer don't go into a prediction, until the last layer makes a prediction. Each layer first combines the outputs from the previous layer (a.k.a. activations) linearly using weights, and then applies an activation function.\n",
    "\n",
    "The neural net we are implementing here looks like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"Net for DLI exercise.drawio.png\"> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Finish the definition of this model. The `__init__` method should contain:\n",
    "- one hidden layer with sigmoid activation functions (this is the \"hidden\" layer of the model, between inputs and outputs, and the number of nodes in it is a variable `hidden_size` that we should be able to set when we initialize the model)\n",
    "- one softmax output layer.\n",
    "2. In the forward pass, the model should do the following:\n",
    " - compute the z values for the hidden layer (linear combinations of the inputs, using the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) neural net building block)\n",
    " - pass them through the [sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) activation function\n",
    " - compute the z values for the output layer (linear combinations of the hidden layer outputs, a.k.a. its activations)\n",
    " - pass them through the Softmax activation function\n",
    "3. Try initializing and inspecting the model as a toy_model instance. It should have 4 features, hidden_size 8, and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exercise template\n",
    "class SimpleNN(torch.nn.Module):\n",
    "\n",
    "    # initializing the model with a certain number of input features\n",
    "    # output classes, and size of hidden layer\n",
    "    def __init__(self, n_features, hidden_size, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # creating one hidden layer h1\n",
    "        # that first applies a linear transformation to the incoming data: z1 = w1T * x\n",
    "        self.h1_linear = torch.nn.Linear(n_features, hidden_size)\n",
    "\n",
    "        # creating the sigmoid activation function for the hidden layer\n",
    "        self.h1_activ = torch.nn.Sigmoid()\n",
    "\n",
    "        # creating one output layer\n",
    "        # that again applies a linear transformation to the incoming activations: z2 = w2T * a1\n",
    "        self.output_linear = torch.nn.Linear(hidden_size,n_classes)\n",
    "        \n",
    "        # creating a softmax activation function for the output layer\n",
    "        self.output_activ = torch.nn.Softmax() \n",
    "\n",
    "    # you have to define the forward() method which will specify the forward propagation:\n",
    "    # how the input values get to the next layer(s)\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # compute the z-values of the hidden layer\n",
    "        z1 = self.h1_linear(inputs)\n",
    "\n",
    "        # pass them through the sigmoid activation function\n",
    "        a1 = self.h1_activ(z1)\n",
    "\n",
    "        #compute the z-values of the output layer\n",
    "        z2 = self.output_linear(a1)\n",
    "        \n",
    "        # get the final values\n",
    "        outputs = self.output_activ(z2)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (h1_linear): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (h1_activ): Sigmoid()\n",
       "  (output_linear): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (output_activ): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toymodel = SimpleNN(4,8,3)\n",
    "toymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
