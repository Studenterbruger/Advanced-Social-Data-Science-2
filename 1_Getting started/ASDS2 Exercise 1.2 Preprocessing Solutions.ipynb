{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jacdals97/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jacdals97/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jacdals97/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jacdals97/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "#You may need to download the following to run this code: \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer # Porter is used below. This is an alternative, harsher stemmer. \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Social Data Science 2 (ASDS2) Exercises\n",
    "\n",
    "\n",
    "## April 21: Preprocessing\n",
    "\n",
    "### 1: Importing data without preprocessing\n",
    "\n",
    "1. Download the data set available here, which contains the nearly 6,000 times Donald Trump insulted someone on Twitter: https://www.kaggle.com/ayushggarg/all-trumps-twitter-insults-20152021 \n",
    "2. Load the csv as a data frame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = pd.read_csv('trump_insult_tweets_2014_to_2021.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>fool</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>DOPE</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>politicians</td>\n",
       "      <td>all talk and no action</td>\n",
       "      <td>Big time in U.S. today - MAKE AMERICA GREAT AG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>ben-cardin</td>\n",
       "      <td>It's politicians like Cardin that have destroy...</td>\n",
       "      <td>Politician @SenatorCardin didn't like that I s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>neil-young</td>\n",
       "      <td>total hypocrite</td>\n",
       "      <td>For the nonbeliever, here is a photo of @Neily...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date          target  \\\n",
       "1  2014-10-09  thomas-frieden   \n",
       "2  2014-10-09  thomas-frieden   \n",
       "3  2015-06-16     politicians   \n",
       "4  2015-06-24      ben-cardin   \n",
       "5  2015-06-24      neil-young   \n",
       "\n",
       "                                              insult  \\\n",
       "1                                               fool   \n",
       "2                                               DOPE   \n",
       "3                             all talk and no action   \n",
       "4  It's politicians like Cardin that have destroy...   \n",
       "5                                    total hypocrite   \n",
       "\n",
       "                                               tweet  \n",
       "1  Can you believe this fool, Dr. Thomas Frieden ...  \n",
       "2  Can you believe this fool, Dr. Thomas Frieden ...  \n",
       "3  Big time in U.S. today - MAKE AMERICA GREAT AG...  \n",
       "4  Politician @SenatorCardin didn't like that I s...  \n",
       "5  For the nonbeliever, here is a photo of @Neily...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The variable ‘target’ has an indicator for the target of the insult. The data reveals that Trump’s most frequent insult target is ‘the media’ (‘the-media’ in the data). Create a binary indicator for whether Trump targets the media. Fit a linear regression with this binary indicator as the dependent variable and the date of the tweet as the independent variable. Does Trump become more or less likely to insult the media over time? Why might this be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the-media                  1287\n",
       "democrats                   647\n",
       "hillary-clinton             625\n",
       "trump-russia                441\n",
       "joe-biden                   402\n",
       "                           ... \n",
       "mccabe-memos                  1\n",
       "state-department              1\n",
       "us-mexico-trade-surplus       1\n",
       "us-court-system               1\n",
       "mike-pence                    1\n",
       "Name: target, Length: 866, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using value counts to see that Trump's most frequent insult target is the media\n",
    "trump['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column with a binary indicator for whether the media is targeted in a given tweet\n",
    "trump['media'] = (trump['target'] == 'the-media').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date is str type, which we cannot use. Correcting date to datetime type\n",
    "trump['date'] = pd.to_datetime(trump['date'], format = '%Y-%m-%d', errors = 'ignore')\n",
    "\n",
    "#make dates regression appropriate:\n",
    "trump['date_ordinal'] = trump['date'].map(dt.datetime.toordinal) #makes each Y-M-D a unique number (starting from some random number)\n",
    "trump['date_from_0'] =  trump['date_ordinal'] - min(trump['date_ordinal']) #Makes each data a number starting from 0 to the number of days since the first tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3098811591001265e-05\n"
     ]
    }
   ],
   "source": [
    "#Create the feature and target vectors\n",
    "#X = trump['date'].values.reshape(-1, 1)\n",
    "X = trump['date_from_0'].values.reshape(-1, 1)\n",
    "\n",
    "y = trump['media'].values\n",
    "\n",
    "#Creating and fitting a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "#Viewing the computed linear regression coefficient to determine whether Trump becomes more or less likely to insult the media over time\n",
    "print(model.coef_[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient is (very very slightly) positive, showing that Trump becomes (very very slightly) more likely to insult the media over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the CountVectorizer from sklearn, convert the tweets to a document-feature matrix. What are the dimensions of the matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10360, 12902)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating and fitting a vectorizer to convert the tweets to a document-feature matrix\n",
    "vectorizer = CountVectorizer(lowercase=False, ngram_range=(1,1), analyzer = \"word\")\n",
    "\n",
    "matrix = vectorizer.fit_transform(trump['tweet'])\n",
    "\n",
    "#Viewing the matrix dimensions\n",
    "matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are interested, you can see the bag of words as identified by the vectorizer by uncommenting below\n",
    "#vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Preprocessing steps\n",
    "\n",
    "1. Remove all tagged users, i.e. words starting with the ‘@’ character.\n",
    "2. Lowercase all tweet text.\n",
    "3. Remove numbers.\n",
    "4. Remove extra whitespace.\n",
    "5. Remove default stopwords.\n",
    "6. Remove punctuation. \n",
    "7. Stem words.\n",
    "8. Lemmatize words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all tagged users, i.e. words starting with the ‘@’ character.\n",
    "trump['tweet_no_tags'] = trump['tweet'].str.replace(r'@\\w+ ', '', regex = True)\n",
    "    #Notice that there may be issues: If Trump mistakenly put a space after @ (e.g. '@ CBS' in index 576) the tag is not removed\n",
    "\n",
    "#Lowercase all tweet text\n",
    "trump['tweet_lowercase'] = trump['tweet_no_tags'].str.lower()\n",
    "\n",
    "#Remove numbers \n",
    "trump['tweet_no_numbers'] = trump['tweet_lowercase'].str.replace(r'\\d+', '', regex = True)\n",
    "\n",
    "#Remove whitespace (a two stage removal process is required to remove all whitespace)\n",
    "trump['tweet_no_whitespace'] = trump['tweet_no_numbers'].str.replace(r'\\s+', ' ', regex = True) #replace all whitespace with a single space\n",
    "trump['tweet_no_whitespace'] = trump['tweet_no_whitespace'].str.strip() #removes whitespace at the beginning and end of the string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "      <th>media</th>\n",
       "      <th>date_ordinal</th>\n",
       "      <th>date_from_0</th>\n",
       "      <th>tweet_no_tags</th>\n",
       "      <th>tweet_lowercase</th>\n",
       "      <th>tweet_no_numbers</th>\n",
       "      <th>tweet_no_whitespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>fool</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>735515</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>DOPE</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>735515</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>politicians</td>\n",
       "      <td>all talk and no action</td>\n",
       "      <td>Big time in U.S. today - MAKE AMERICA GREAT AG...</td>\n",
       "      <td>0</td>\n",
       "      <td>735765</td>\n",
       "      <td>250</td>\n",
       "      <td>Big time in U.S. today - MAKE AMERICA GREAT AG...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>ben-cardin</td>\n",
       "      <td>It's politicians like Cardin that have destroy...</td>\n",
       "      <td>Politician @SenatorCardin didn't like that I s...</td>\n",
       "      <td>0</td>\n",
       "      <td>735773</td>\n",
       "      <td>258</td>\n",
       "      <td>Politician didn't like that I said Baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>neil-young</td>\n",
       "      <td>total hypocrite</td>\n",
       "      <td>For the nonbeliever, here is a photo of @Neily...</td>\n",
       "      <td>0</td>\n",
       "      <td>735773</td>\n",
       "      <td>258</td>\n",
       "      <td>For the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date          target  \\\n",
       "1 2014-10-09  thomas-frieden   \n",
       "2 2014-10-09  thomas-frieden   \n",
       "3 2015-06-16     politicians   \n",
       "4 2015-06-24      ben-cardin   \n",
       "5 2015-06-24      neil-young   \n",
       "\n",
       "                                              insult  \\\n",
       "1                                               fool   \n",
       "2                                               DOPE   \n",
       "3                             all talk and no action   \n",
       "4  It's politicians like Cardin that have destroy...   \n",
       "5                                    total hypocrite   \n",
       "\n",
       "                                               tweet  media  date_ordinal  \\\n",
       "1  Can you believe this fool, Dr. Thomas Frieden ...      0        735515   \n",
       "2  Can you believe this fool, Dr. Thomas Frieden ...      0        735515   \n",
       "3  Big time in U.S. today - MAKE AMERICA GREAT AG...      0        735765   \n",
       "4  Politician @SenatorCardin didn't like that I s...      0        735773   \n",
       "5  For the nonbeliever, here is a photo of @Neily...      0        735773   \n",
       "\n",
       "   date_from_0                                      tweet_no_tags  \\\n",
       "1            0  Can you believe this fool, Dr. Thomas Frieden ...   \n",
       "2            0  Can you believe this fool, Dr. Thomas Frieden ...   \n",
       "3          250  Big time in U.S. today - MAKE AMERICA GREAT AG...   \n",
       "4          258  Politician didn't like that I said Baltimore n...   \n",
       "5          258  For the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                     tweet_lowercase  \\\n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  can you believe this fool, dr. thomas frieden ...   \n",
       "3  big time in u.s. today - make america great ag...   \n",
       "4  politician didn't like that i said baltimore n...   \n",
       "5  for the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                    tweet_no_numbers  \\\n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  can you believe this fool, dr. thomas frieden ...   \n",
       "3  big time in u.s. today - make america great ag...   \n",
       "4  politician didn't like that i said baltimore n...   \n",
       "5  for the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                 tweet_no_whitespace  \n",
       "1  can you believe this fool, dr. thomas frieden ...  \n",
       "2  can you believe this fool, dr. thomas frieden ...  \n",
       "3  big time in u.s. today - make america great ag...  \n",
       "4  politician didn't like that i said baltimore n...  \n",
       "5  for the nonbeliever, here is a photo of in my ...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing preprocesing steps so far\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Viewing imported stopwords to be removed\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "#Notice that they include words like \"didn't\" with an apostrophe. \n",
    "#If we remove punctuation before removing stopwords, a word like \"didn't\" would not be recognized and removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple function to remove stopwords\n",
    "\n",
    "#Remove default stopwords\n",
    "def remove_stopwords(sent):\n",
    "    \n",
    "    patterns = set(stopwords.words('english'))\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if re.search('\\\\b'+pattern+'\\\\b', sent):           #Searching for exact match of stopwords in each tweet\n",
    "            sent = re.sub('\\\\b'+pattern+'\\\\b', '', sent)   #Substituting stopwords with empty string\n",
    "    sent = re.sub(r'\\s+',' ',sent)                           #Removing stopwords creates whitespace which we remove here\n",
    "    sent = sent.strip()\n",
    "    return sent\n",
    "\n",
    "\n",
    "trump['tweet_no_stopwords'] = trump['tweet_no_whitespace'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b(i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't)\\b\n"
     ]
    }
   ],
   "source": [
    "#Advanced function to remove stopwords\n",
    "\n",
    "#Instead of looping through stopwords, we can use a regular expression to match all stopwords at once\n",
    "stopword_match = r\"\\b(\" + r\"|\".join(stopwords.words('english')) + r\")\\b\"\n",
    "print(stopword_match)\n",
    "#Notice that the regular expression is a bit more complicated than the simple function above\n",
    "\n",
    "trump['tweet_no_stopwords'] = trump['tweet_no_whitespace'].str.replace(stopword_match, '', regex = True).str.replace(r'\\s+', ' ', regex = True).str.strip()\n",
    "#Notice we are simply chaining the three steps from the simple function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation \n",
    "trump['tweet_no_punc'] = trump['tweet_no_stopwords'].str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem words\n",
    "\n",
    "def stemmer(sent, stemmer = PorterStemmer()):\n",
    "    \n",
    "    sent = word_tokenize(sent)        #Tokenizing, as stemmer only takes tokenized sentences\n",
    "    sent_stemmed = [stemmer.stem(word) for word in sent]       #Stemming each word in the sentence with list comprehension\n",
    "    return ' '.join(sent_stemmed)    #Joining the stemmed words back into a sentence\n",
    "\n",
    "trump['tweet_stemmed'] = trump['tweet_no_punc'].apply(lambda x: stemmer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize words \n",
    "\n",
    "def lemmatize(sent, lemmatizer = WordNetLemmatizer()):\n",
    "    \n",
    "    #First, the nltk wordnet lemmatizer needs the part-of-speech (POS) tag to correctly lemmatize\n",
    "    #NLTK has a POS-tagger, but the format does not match POS-tags in wordnet's lemmatizer. \n",
    "    #The mapping dictionary below fixes that.\n",
    "    \n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)  #If nothing else is specified, use noun tag\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV    \n",
    "    \n",
    "    sent = word_tokenize(sent)              #Tokenizing, as lemmatizer only takes tokenized sentences\n",
    "    sent_lemmatized = [lemmatizer.lemmatize(word, tag_map[tag[0]]) for word, tag in pos_tag(sent)]  #Lemmatizing each word in the sentence with list comprehension        \n",
    "    #Notice above that we choose tag[0] to get all instances of a word class, \n",
    "    #e.g. NN (noun) and NP (proper noun) should both translate to noun. \n",
    "    \n",
    "    return ' '.join(sent_lemmatized)\n",
    "\n",
    "trump['tweet_lemmatized'] = trump['tweet_no_punc'].apply(lambda x: lemmatize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "      <th>media</th>\n",
       "      <th>date_ordinal</th>\n",
       "      <th>date_from_0</th>\n",
       "      <th>tweet_no_tags</th>\n",
       "      <th>tweet_lowercase</th>\n",
       "      <th>tweet_no_numbers</th>\n",
       "      <th>tweet_no_whitespace</th>\n",
       "      <th>tweet_no_stopwords</th>\n",
       "      <th>tweet_no_punc</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>fool</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>735515</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>believe fool, dr. thomas frieden cdc, stated, ...</td>\n",
       "      <td>believe fool dr thomas frieden cdc stated anyo...</td>\n",
       "      <td>believ fool dr thoma frieden cdc state anyon f...</td>\n",
       "      <td>believe fool dr thomas frieden cdc state anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-10-09</td>\n",
       "      <td>thomas-frieden</td>\n",
       "      <td>DOPE</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>735515</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you believe this fool, Dr. Thomas Frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>can you believe this fool, dr. thomas frieden ...</td>\n",
       "      <td>believe fool, dr. thomas frieden cdc, stated, ...</td>\n",
       "      <td>believe fool dr thomas frieden cdc stated anyo...</td>\n",
       "      <td>believ fool dr thoma frieden cdc state anyon f...</td>\n",
       "      <td>believe fool dr thomas frieden cdc state anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>politicians</td>\n",
       "      <td>all talk and no action</td>\n",
       "      <td>Big time in U.S. today - MAKE AMERICA GREAT AG...</td>\n",
       "      <td>0</td>\n",
       "      <td>735765</td>\n",
       "      <td>250</td>\n",
       "      <td>Big time in U.S. today - MAKE AMERICA GREAT AG...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>big time in u.s. today - make america great ag...</td>\n",
       "      <td>big time u.. today - make america great ! poli...</td>\n",
       "      <td>big time u today  make america great  politici...</td>\n",
       "      <td>big time u today make america great politician...</td>\n",
       "      <td>big time u today make america great politician...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>ben-cardin</td>\n",
       "      <td>It's politicians like Cardin that have destroy...</td>\n",
       "      <td>Politician @SenatorCardin didn't like that I s...</td>\n",
       "      <td>0</td>\n",
       "      <td>735773</td>\n",
       "      <td>258</td>\n",
       "      <td>Politician didn't like that I said Baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "      <td>politician didn't like that i said baltimore n...</td>\n",
       "      <td>politician ' like said baltimore needs jobs &amp; ...</td>\n",
       "      <td>politician  like said baltimore needs jobs  sp...</td>\n",
       "      <td>politician like said baltimor need job spirit ...</td>\n",
       "      <td>politician like say baltimore need job spirit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-24</td>\n",
       "      <td>neil-young</td>\n",
       "      <td>total hypocrite</td>\n",
       "      <td>For the nonbeliever, here is a photo of @Neily...</td>\n",
       "      <td>0</td>\n",
       "      <td>735773</td>\n",
       "      <td>258</td>\n",
       "      <td>For the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>for the nonbeliever, here is a photo of in my ...</td>\n",
       "      <td>nonbeliever, photo office $$ request—total hyp...</td>\n",
       "      <td>nonbeliever photo office  request—total hypocr...</td>\n",
       "      <td>nonbeliev photo offic request—tot hypocrit htt...</td>\n",
       "      <td>nonbeliever photo office request—total hypocri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date          target  \\\n",
       "1 2014-10-09  thomas-frieden   \n",
       "2 2014-10-09  thomas-frieden   \n",
       "3 2015-06-16     politicians   \n",
       "4 2015-06-24      ben-cardin   \n",
       "5 2015-06-24      neil-young   \n",
       "\n",
       "                                              insult  \\\n",
       "1                                               fool   \n",
       "2                                               DOPE   \n",
       "3                             all talk and no action   \n",
       "4  It's politicians like Cardin that have destroy...   \n",
       "5                                    total hypocrite   \n",
       "\n",
       "                                               tweet  media  date_ordinal  \\\n",
       "1  Can you believe this fool, Dr. Thomas Frieden ...      0        735515   \n",
       "2  Can you believe this fool, Dr. Thomas Frieden ...      0        735515   \n",
       "3  Big time in U.S. today - MAKE AMERICA GREAT AG...      0        735765   \n",
       "4  Politician @SenatorCardin didn't like that I s...      0        735773   \n",
       "5  For the nonbeliever, here is a photo of @Neily...      0        735773   \n",
       "\n",
       "   date_from_0                                      tweet_no_tags  \\\n",
       "1            0  Can you believe this fool, Dr. Thomas Frieden ...   \n",
       "2            0  Can you believe this fool, Dr. Thomas Frieden ...   \n",
       "3          250  Big time in U.S. today - MAKE AMERICA GREAT AG...   \n",
       "4          258  Politician didn't like that I said Baltimore n...   \n",
       "5          258  For the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                     tweet_lowercase  \\\n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  can you believe this fool, dr. thomas frieden ...   \n",
       "3  big time in u.s. today - make america great ag...   \n",
       "4  politician didn't like that i said baltimore n...   \n",
       "5  for the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                    tweet_no_numbers  \\\n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  can you believe this fool, dr. thomas frieden ...   \n",
       "3  big time in u.s. today - make america great ag...   \n",
       "4  politician didn't like that i said baltimore n...   \n",
       "5  for the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                 tweet_no_whitespace  \\\n",
       "1  can you believe this fool, dr. thomas frieden ...   \n",
       "2  can you believe this fool, dr. thomas frieden ...   \n",
       "3  big time in u.s. today - make america great ag...   \n",
       "4  politician didn't like that i said baltimore n...   \n",
       "5  for the nonbeliever, here is a photo of in my ...   \n",
       "\n",
       "                                  tweet_no_stopwords  \\\n",
       "1  believe fool, dr. thomas frieden cdc, stated, ...   \n",
       "2  believe fool, dr. thomas frieden cdc, stated, ...   \n",
       "3  big time u.. today - make america great ! poli...   \n",
       "4  politician ' like said baltimore needs jobs & ...   \n",
       "5  nonbeliever, photo office $$ request—total hyp...   \n",
       "\n",
       "                                       tweet_no_punc  \\\n",
       "1  believe fool dr thomas frieden cdc stated anyo...   \n",
       "2  believe fool dr thomas frieden cdc stated anyo...   \n",
       "3  big time u today  make america great  politici...   \n",
       "4  politician  like said baltimore needs jobs  sp...   \n",
       "5  nonbeliever photo office  request—total hypocr...   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "1  believ fool dr thoma frieden cdc state anyon f...   \n",
       "2  believ fool dr thoma frieden cdc state anyon f...   \n",
       "3  big time u today make america great politician...   \n",
       "4  politician like said baltimor need job spirit ...   \n",
       "5  nonbeliev photo offic request—tot hypocrit htt...   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "1  believe fool dr thomas frieden cdc state anyon...  \n",
       "2  believe fool dr thomas frieden cdc state anyon...  \n",
       "3  big time u today make america great politician...  \n",
       "4  politician like say baltimore need job spirit ...  \n",
       "5  nonbeliever photo office request—total hypocri...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing dataset with all preprocessing steps \n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Consequences of preprocessing\n",
    "\n",
    "Create a new document-feature matrix with the preprocessed tweets. How do the dimensions of this matrix compare with those of the matrix you created in 1.3?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10360, 8158)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(trump['tweet_lemmatized'])\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10360, 7170)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(trump['tweet_stemmed'])\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the new bag of words, uncomment below\n",
    "#vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
