{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clara/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# enabling inline plots in Jupyter\n",
    "%matplotlib inline\n",
    "# disabling verbose messages from dataset library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classification II\n",
    "\n",
    "In this exercise session, you will be using cross-validation to check the out-of-sample performance of different models for classifying movie review sentiment (using TF-IDF features as in the Classification I problem set). You will compare a logistic regression model to SVM and Naive Bayes. You will also use cross-validation for hyperparameter grid search.\n",
    "\n",
    "Pro tip: As you will be fitting a lot of models in this exercise, why not take a look at how sklearn handles [parallelism](https://scikit-learn.org/stable/computing/parallelism.html#parallelism). A lot of method in the sklearn library take a parameter [n_jobs](https://scikit-learn.org/stable/glossary.html#term-n_jobs). By setting it to -1, you can use all of your CPUs (cores) at once. Depending on your hardware you may see 8x faster code, which means less waiting and more learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Cross-validation\n",
    "\n",
    "Cross-validate the logistic regression classifier on the `rotten_tomatoes` dataset with TF-IDF vectorization that we used in the previous exercise. Perform 5-fold stratified cross-validation with the built-in method `cross_val_score` method in the sklearn `model_selection` module. Throughout this exercise (up to step 6), set the `scoring` parameter of `cross_val_score` to \"accuracy\". This means we will be using accuracy as our performance metric.\n",
    "\n",
    "Compare performance (averaged across the five folds) to the model's in-sample performance on the training set. Does the model seem to be overfitting?\n",
    "\n",
    "\n",
    "Reference: sklearn `cross_val_score` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Regularization\n",
    "\n",
    "Look up the documentation for [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). What parameters related to regularization are there?\n",
    "\n",
    "1. Add the regularization term to the logistic regression classifier with L2 regularization and retrain it. Set the value of the regularization parameter to any non-default value within its range.\n",
    "2. Compare the cross-validation performance to the unregularized classifier. Did anything change? Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Hyperparameter search\n",
    "\n",
    "1. Is the default value for the regularization parameter the best possible one? Use grid search with cross-validation to try several options.\n",
    "2. What is your best model? Compare its cross-validation performance to that of the original, non-regularized model.\n",
    "\n",
    "Reference: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. SVM classifier\n",
    "\n",
    "Perform the same experiment with the LinearSVC classifier (this is an SVM with a linear kernel) on the *rotten_tomatoes* dataset.\n",
    "\n",
    "1. Start with the default parameter settings.\n",
    "2. Try to find the best option for the *c* hyperparameter with grid search. What is your best model performance?\n",
    "3. Optional: try the SVM with a non-linear RBF kernel, and do the hyperparameter search on both *gamma* and *c*.\n",
    "\n",
    "Documentation for the LinearSVC classifier: [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "More about SVMs: [link](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Naive Bayes classifier\n",
    "\n",
    "Perform the same experiment with the Naive Bayes classifier. You can use a Multinomial Naive Bayes model (`MultinomialNB`) here with default parameter settings, as this is the variant that we covered in class (predicting categories from word occurence counts).\n",
    "\n",
    "1. Multinomial Naive Bayes models don't take TF-IDF features, but rather word occurrence counts (so we need to leave out the IDF step). For that reason, re-vectorize the training data and then the test data using the `sklearn` `CountVectorizer` instead.\n",
    "2. Run the model on the count-vectorized training data. You don't need to do a hyperparameter grid search. What is your model performance?\n",
    "\n",
    "Documentation for the MultinomialNB classifier: [link](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "More about Naive Bayes models in sklearn: [link](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "Note: it seems the behavior of the classifier can be unstable when using n_jobs=-1. It should be fast enough without it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Comparative analysis of classifier performance\n",
    "\n",
    "1. Use the code below as a starting point to compare the performance of logistic regression, Linear SVC and Naive Bayes classifers (with the best hyperparameters you could find for the first two models, and using the count-vectorized test data for the Naive Bayes classifier). Use both accuracy and F1 metrics. Are the two metrics consistent? Which is the best-performing model?\n",
    "2. Bonus: evaluate your three classifiers on your small test dataset that you annotated yourself in Classification I class. Are all the classifiers behaving the same way?\n",
    "\n",
    "Note: to get the best performing model, you can take the result of `GridSearchCV` and use its attribute `.best_estimator_`. Then, to use that model to make predictions on a new data set, you can apply the `.predict()` method to the model, giving it the new data set's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
