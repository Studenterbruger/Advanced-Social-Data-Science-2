{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clara/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, logging\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# enabling inline plots in Jupyter\n",
    "%matplotlib inline\n",
    "# disabling verbose messages from dataset library\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Classification II\n",
    "\n",
    "In this exercise session, you will be using cross-validation to check the out-of-sample performance of different models for classifying movie review sentiment (using TF-IDF features as in the Classification I problem set). You will compare a logistic regression model to SVM and Naive Bayes. You will also use cross-validation for hyperparameter grid search.\n",
    "\n",
    "Pro tip: As you will be fitting a lot of models in this exercise, why not take a look at how sklearn handles [parallelism](https://scikit-learn.org/stable/computing/parallelism.html#parallelism). A lot of method in the sklearn library take a parameter [n_jobs](https://scikit-learn.org/stable/glossary.html#term-n_jobs). By setting it to -1, you can use all of your CPUs (cores) at once. Depending on your hardware you may see 8x faster code, which means less waiting and more learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Cross-validation\n",
    "\n",
    "Cross-validate the logistic regression classifier on the `rotten_tomatoes` dataset with TF-IDF vectorization that we used in the previous exercise. Perform 5-fold stratified cross-validation with the built-in method `cross_val_score` method in the sklearn `model_selection` module. Throughout this exercise (up to step 6), set the `scoring` parameter of `cross_val_score` to \"accuracy\". This means we will be using accuracy as our performance metric.\n",
    "\n",
    "Compare performance (averaged across the five folds) to the model's in-sample performance on the training set. Does the model seem to be overfitting?\n",
    "\n",
    "\n",
    "Reference: sklearn `cross_val_score` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the 2-class sentiment classification model from rotten tomatoes\n",
    "train = load_dataset('rotten_tomatoes', split='train')\n",
    "val = load_dataset('rotten_tomatoes',  split='validation')\n",
    "test = load_dataset('rotten_tomatoes', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizing the data with TF-IDF corpus\n",
    "vectorizer = TfidfVectorizer() # the default ngram range is (1,1)\n",
    "\n",
    "train_corpus = [x[\"text\"] for x in train]\n",
    "train_labels = [x[\"label\"] for x in train]\n",
    "train_features = vectorizer.fit_transform(train_corpus)\n",
    "\n",
    "val_corpus = [x[\"text\"] for x in val]\n",
    "val_labels = [x[\"label\"] for x in val]\n",
    "val_features = vectorizer.transform(val_corpus)\n",
    "\n",
    "test_corpus = [x[\"text\"] for x in test]\n",
    "test_labels = [x[\"label\"] for x in test]\n",
    "test_features = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression in-sample accuracy:  0.8960140679953107\n",
      "Logistic Regression cross-validation accuracy:  0.7514654161781946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Perform 5-fold stratified cross-validation with the built-in method `cross_val_score` method\n",
    "lr_score = cross_val_score(LogisticRegression(), train_features, train_labels, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "#Compare performance (averaged across the five folds) to the model's in-sample performance on the training set\n",
    "lr = LogisticRegression().fit(train_features, train_labels)\n",
    "preds = lr.predict(train_features)\n",
    "\n",
    "print(\"Logistic Regression in-sample accuracy: \", accuracy_score(train_labels, preds))\n",
    "print(\"Logistic Regression cross-validation accuracy: \", np.mean(lr_score))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Regularization\n",
    "\n",
    "Look up the documentation for [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). What parameters related to regularization are there?\n",
    "\n",
    "1. Add the regularization term to the logistic regression classifier with L2 regularization and retrain it. Set the value of the regularization parameter to any non-default value within its range.\n",
    "2. Compare the cross-validation performance to the unregularized classifier. Did anything change? Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Logistic Regression in-sample accuracy:  0.8575615474794842\n",
      "Regularized Logistic Regression cross-validation accuracy:  0.7403282532239155\n",
      "Logistic Regression cross-validation accuracy:  0.7514654161781946\n"
     ]
    }
   ],
   "source": [
    "lr_score_reg = cross_val_score(LogisticRegression( C=0.5, penalty=\"l2\"), train_features, train_labels, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "lr = LogisticRegression(C=0.5, penalty=\"l2\").fit(train_features, train_labels)\n",
    "preds = lr.predict(train_features)\n",
    "\n",
    "print(\"Regularized Logistic Regression in-sample accuracy: \", accuracy_score(train_labels, preds))\n",
    "print(\"Regularized Logistic Regression cross-validation accuracy: \", np.mean(lr_score_reg))\n",
    "print(\"Logistic Regression cross-validation accuracy: \", np.mean(lr_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Hyperparameter search\n",
    "\n",
    "1. Is the default value for the regularization parameter the best possible one? Use grid search with cross-validation to try several options.\n",
    "2. What is your best model? Compare its cross-validation performance to that of the original, non-regularized model.\n",
    "\n",
    "Reference: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.76\n",
      "Best parameters:  {'C': 10}\n",
      "Best estimator:  LogisticRegression(C=10)\n",
      "Logistic Regression cross-validation accuracy:  0.7514654161781946\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV is a module that enables running\n",
    "# cross-validated grid-search over a parameter grid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# the parameters to explore are passed as param_grid parameter\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 0.5, 0.8, 1, 10]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(penalty=\"l2\"), param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "lr_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)\n",
    "print(\"Best estimator: \", lr_grid.best_estimator_)\n",
    "\n",
    "#Compare its cross-validation performance to that of the original, non-regularized model.\n",
    "print(\"Logistic Regression cross-validation accuracy: \", np.mean(lr_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. SVM classifier\n",
    "\n",
    "Perform the same experiment with the LinearSVC classifier (this is an SVM with a linear kernel) on the *rotten_tomatoes* dataset.\n",
    "\n",
    "1. Start with the default parameter settings.\n",
    "2. Try to find the best option for the *c* hyperparameter with grid search. What is your best model performance?\n",
    "3. Optional: try the SVM with a non-linear RBF kernel, and do the hyperparameter search on both *gamma* and *c*.\n",
    "\n",
    "Documentation for the LinearSVC classifier: [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "More about SVMs: [link](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC cross-validation accuracy:  0.7566236811254397\n"
     ]
    }
   ],
   "source": [
    "# default LinearSVC\n",
    "linSVC = LinearSVC()\n",
    "\n",
    "linSVC_score = cross_val_score(LinearSVC(), train_features, train_labels, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", n_jobs=-1)\n",
    "print(\"LinearSVC cross-validation accuracy: \", np.mean(linSVC_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.76\n",
      "Best parameters:  {'C': 1}\n",
      "Best estimator:  LinearSVC(C=1)\n",
      "LinearSVC cross-validation accuracy:  0.7566236811254397\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter search on LinearSVC\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "linSVC_grid = GridSearchCV(LinearSVC(), param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "linSVC_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(linSVC_grid.best_score_))\n",
    "print(\"Best parameters: \", linSVC_grid.best_params_)\n",
    "print(\"Best estimator: \", linSVC_grid.best_estimator_)\n",
    "print(\"LinearSVC cross-validation accuracy: \", np.mean(linSVC_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best cross-validation score: 0.76\n",
      "Best parameters:  {'C': 10, 'gamma': 0.1}\n",
      "Best estimator:  SVC(C=10, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter search on SVC with rbf kernel\n",
    "param_grid = {'C': [0.001, 0.1, 10], 'gamma': [0.001, 0.1, 10]}\n",
    "svc_grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "svc_grid.fit(train_features, train_labels)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(svc_grid.best_score_))\n",
    "print(\"Best parameters: \", svc_grid.best_params_)\n",
    "print(\"Best estimator: \", svc_grid.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Naive Bayes classifier\n",
    "\n",
    "Perform the same experiment with the Naive Bayes classifier. You can use a Multinomial Naive Bayes model (`MultinomialNB`) here with default parameter settings, as this is the variant that we covered in class (predicting categories from word occurence counts).\n",
    "\n",
    "1. Multinomial Naive Bayes models don't take TF-IDF features, but rather word occurrence counts (so we need to leave out the IDF step). For that reason, re-vectorize the training data and then the test data using the `sklearn` `CountVectorizer` instead.\n",
    "2. Run the model on the count-vectorized training data. You don't need to do a hyperparameter grid search. What is your model performance?\n",
    "\n",
    "Documentation for the MultinomialNB classifier: [link](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "More about Naive Bayes models in sklearn: [link](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "`Note` it seems the behavior of the classifier can be unstable when using n_jobs=-1. It should be fast enough without it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   1.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   1.6s\n",
      "[CV] END .................................................... total time=   1.7s\n",
      "[CV] END .................................................... total time=   1.5s\n",
      "[CV] END .................................................... total time=   1.5s\n",
      "Naive Bayes cross-validation accuracy:  0.7648300117233294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "counterizer = CountVectorizer()\n",
    "train_counts = counterizer.fit_transform(train_corpus)\n",
    "test_counts = counterizer.transform(test_corpus)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb_score = cross_val_score(nb, train_counts.toarray(), train_labels, cv=StratifiedKFold(n_splits=5), scoring=\"accuracy\", verbose=2)\n",
    "\n",
    "print(\"Naive Bayes cross-validation accuracy: \", np.mean(nb_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Comparative analysis of classifier performance\n",
    "\n",
    "1. Compare the performance of logistic regression, Linear SVC and Naive Bayes classifers (with the best hyperparameters you could find for the first two models, and using the count-vectorized test data for the Naive Bayes classifier). Use both accuracy and F1 metrics. Are the two metrics consistent? Which is the best-performing model?\n",
    "2. Bonus: evaluate your three classifiers on your small test dataset that you annotated yourself in Classification I class. Are all the classifiers behaving the same way?\n",
    "\n",
    "Note: to get the best performing model, you can take the result of `GridSearchCV` and use its attribute `.best_estimator_`. Then, to use that model to make predictions on a new data set, you can apply the `.predict()` method to the model, giving it the new data set's features as an argument."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## F1 score and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting LR and LinearSVC predictions\n",
    "\n",
    "lr_test_preds = lr_grid.best_estimator_.predict(test_features)\n",
    "linSVC_test_preds = linSVC_grid.best_estimator_.predict(test_features)\n",
    "\n",
    "# getting NB predictions\n",
    "nb.fit(train_counts.toarray(), train_labels)\n",
    "nb_test_preds = nb.predict(test_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# collecting the accuracy data data\n",
    "results = {\"Accuracy\":dict(),\"F1 score\":dict()}\n",
    "results[\"Accuracy\"][\"LR\"] = accuracy_score(test_labels, lr_test_preds)\n",
    "results[\"Accuracy\"][\"SVC\"] = accuracy_score(test_labels, linSVC_test_preds)\n",
    "results[\"Accuracy\"][\"NB\"] = accuracy_score(test_labels, nb_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': {'LR': 0.776735459662289,\n",
       "  'SVC': 0.7729831144465291,\n",
       "  'NB': 0.797373358348968},\n",
       " 'F1 score': {'LR': 0.7766969440923813,\n",
       "  'SVC': 0.7729439515561187,\n",
       "  'NB': 0.7973277000264062}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding F1 data\n",
    "results[\"F1 score\"][\"LR\"] = f1_score(test_labels, lr_test_preds, average=\"macro\")\n",
    "results[\"F1 score\"][\"SVC\"] = f1_score(test_labels, linSVC_test_preds, average=\"macro\")\n",
    "results[\"F1 score\"][\"NB\"] = f1_score(test_labels, nb_test_preds, average=\"macro\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy  F1 score\n",
       "LR      0.777     0.777\n",
       "SVC     0.773     0.773\n",
       "NB      0.797     0.797"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.round(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the logistic regression and linear SVC perform equally well. Both are outperformed by the Naive Bayes model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation on out-of-distribution data\n",
    "\n",
    "We created a short dataset of reviews of the Mario Bros. movie, also taken from Rotten Tomatoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# reading in and vectorizing the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mydata \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../dataset/classification1_annotation.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m mytest_corpus\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(mydata[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m mytest_labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(mydata[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# reading in and vectorizing the data\n",
    "mydata = pd.read_csv(\"../dataset/classification1_annotation.csv\")\n",
    "mytest_corpus= list(mydata[\"text\"])\n",
    "mytest_labels = list(mydata[\"label\"])\n",
    "mytest_features = vectorizer.transform(mytest_corpus)\n",
    "mytest_counts = counterizer.transform(mytest_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 score</th>\n",
       "      <th>OOD accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.773</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy  F1 score  OOD accuracy\n",
       "LR      0.777     0.777         0.667\n",
       "SVC     0.773     0.773         0.667\n",
       "NB      0.797     0.797         0.583"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"OOD accuracy\"] = {}\n",
    "results[\"OOD accuracy\"][\"LR\"] = accuracy_score(mytest_labels, lr_grid.best_estimator_.predict(mytest_features))\n",
    "results[\"OOD accuracy\"][\"SVC\"] = accuracy_score(mytest_labels, linSVC_grid.best_estimator_.predict(mytest_features))\n",
    "results[\"OOD accuracy\"][\"NB\"] = accuracy_score(mytest_labels, nb.predict(mytest_counts.toarray()))\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.round(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "All models have worse performance out-of-sample, but in our hand-annotated data, the Naive Bayes classifier goes down the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
